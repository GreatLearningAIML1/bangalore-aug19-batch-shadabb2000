{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUZjPnVXGz0Z"
   },
   "source": [
    "# The Iris Dataset\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMbmpriavLE9"
   },
   "source": [
    "### Specifying the TensorFlow version\n",
    "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu8bUU__oa7h"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLz1Ckvfvn6D"
   },
   "source": [
    "### Import TensorFlow\n",
    "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1937,
     "status": "ok",
     "timestamp": 1580042214906,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "CWrzVTLOvn6M",
    "outputId": "f2ae409e-d11d-4d55-c90d-8d3ee58e4376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uYeJgkNuXNC"
   },
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcASNsewsfQX"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-vVQBBqg7DI"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kE0EDKvQhEIe"
   },
   "source": [
    "### Import dataset\n",
    "- Import iris dataset\n",
    "- Import the dataset using sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X92JJ6NJdHfu"
   },
   "outputs": [],
   "source": [
    "# # Code to read csv file into Colaboratory:\n",
    "# !pip install -U -q PyDrive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# # Authenticate and create the PyDrive client.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOOWpD26Haq3"
   },
   "outputs": [],
   "source": [
    "# # Import pandas \n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# link = \"https://drive.google.com/open?id=1eYj0HhuFw3hxYk6BoxKOGya0PRMqh5YB\"\n",
    "# fluff, id = link.split('=')\n",
    "# print (id) \n",
    "\n",
    "# downloaded = drive.CreateFile({'id':id}) \n",
    "# downloaded.GetContentFile('iris.csv')  \n",
    "# df = pd.read_csv('iris.csv')\n",
    "# # Dataset is now stored in a Pandas Dataframe\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1916,
     "status": "ok",
     "timestamp": 1580042214911,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "OV6cMWtMezfs",
    "outputId": "6403f023-211f-497e-bad2-c032955f982d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 223,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt \n",
    "# Load digits dataset\n",
    "iris = datasets.load_iris()\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ta8YqInTh5v5"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HERt3drbhX0i"
   },
   "source": [
    "### Get features and label from the dataset in separate variable\n",
    "- you can get the features using .data method\n",
    "- you can get the features using .target method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1909,
     "status": "ok",
     "timestamp": 1580042214912,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "0cV-_qHAHyvE",
    "outputId": "a35536f1-5d47-41f5-c1f3-88c4ccc18cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create feature matrix\n",
    "X = iris.data\n",
    "print(type(X))\n",
    "\n",
    "# Create target vector\n",
    "y = iris.target\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1898,
     "status": "ok",
     "timestamp": 1580042214913,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "m8BCMWunfT0T",
    "outputId": "18d3f18d-d0a6-4bdf-d208-268b59914342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 225,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qg1A2lkUjFak"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### Create train and test data\n",
    "- use train_test_split to get train and test set\n",
    "- set a random_state\n",
    "- test_size: 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYKNJL85h7pQ"
   },
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data up in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0KVP17Ozaix"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIjqxbhWv1zv"
   },
   "source": [
    "### One-hot encode the labels\n",
    "- convert class vectors (integers) to binary class matrix\n",
    "- convert labels\n",
    "- number of classes: 3\n",
    "- we are doing this to use categorical_crossentropy as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9vv-_gpyLY9"
   },
   "outputs": [],
   "source": [
    "#from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#Encoding the output class label (One-Hot Encoding)\n",
    "y_train=to_categorical(y_train,3,dtype='int')\n",
    "y_test=to_categorical(y_test,3,dtype='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1883,
     "status": "ok",
     "timestamp": 1580042214917,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "squ8tXtMiLrL",
    "outputId": "07fa63ac-9216-4140-b8a3-7100bd3fd4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 228,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1876,
     "status": "ok",
     "timestamp": 1580042214918,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "xwrlvf7MiO7p",
    "outputId": "1b2b78fd-b2fe-42d4-fbc3-9fda0a4944ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 229,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovjLyYzWkO9s"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbIFzoPNSyYo"
   },
   "source": [
    "### Initialize a sequential model\n",
    "- Define a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FvSbf1UjHtl"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "import keras\n",
    "#Initialize Sequential Graph (model)\n",
    "model = tf.keras.Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGMy999vlacX"
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72ibK5Jxm8iL"
   },
   "source": [
    "### Add a layer\n",
    "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
    "- Apply Softmax on Dense Layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZKrBNSRm_o9"
   },
   "outputs": [],
   "source": [
    "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
    "model.add(Dense(4, activation='relu', input_shape=(4,)))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4uiTH8plmNX"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJL8n8vcSyYz"
   },
   "source": [
    "### Compile the model\n",
    "- Use SGD as Optimizer\n",
    "- Use categorical_crossentropy as loss function\n",
    "- Use accuracy as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tc_-fjIEk1ve"
   },
   "outputs": [],
   "source": [
    "#Compile the model - add Loss and Gradient Descent optimizer\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sihIGbRll_jT"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54ZZCfNGlu0i"
   },
   "source": [
    "### Summarize the model\n",
    "- Check model layers\n",
    "- Understand number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1852,
     "status": "ok",
     "timestamp": 1580042214922,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "elER3F_4ln8n",
    "outputId": "259bbe98-37e3-4fc7-d892-54c9cd27de99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PiP7j3Vmj4p"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWdbfFCXmCHt"
   },
   "source": [
    "### Fit the model\n",
    "- Give train data as training features and labels\n",
    "- Epochs: 100\n",
    "- Give validation data as testing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5258,
     "status": "ok",
     "timestamp": 1580042218345,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "cO1c-5tjmBVZ",
    "outputId": "5ebc8a53-a7cc-4378-9b4b-9ebbcaa7a6c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112 samples, validate on 38 samples\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 0s 3ms/sample - loss: 1.5174 - accuracy: 0.3482 - val_loss: 1.3837 - val_accuracy: 0.2895\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 0s 211us/sample - loss: 1.3323 - accuracy: 0.3482 - val_loss: 1.2646 - val_accuracy: 0.2895\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 0s 222us/sample - loss: 1.2352 - accuracy: 0.3482 - val_loss: 1.2039 - val_accuracy: 0.2895\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 0s 199us/sample - loss: 1.1816 - accuracy: 0.3482 - val_loss: 1.1702 - val_accuracy: 0.2895\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 0s 188us/sample - loss: 1.1510 - accuracy: 0.3482 - val_loss: 1.1418 - val_accuracy: 0.2895\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 0s 199us/sample - loss: 1.1259 - accuracy: 0.3482 - val_loss: 1.1277 - val_accuracy: 0.2895\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 0s 184us/sample - loss: 1.1121 - accuracy: 0.3482 - val_loss: 1.1177 - val_accuracy: 0.2895\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 0s 207us/sample - loss: 1.1020 - accuracy: 0.3482 - val_loss: 1.1079 - val_accuracy: 0.2895\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 0s 204us/sample - loss: 1.0936 - accuracy: 0.3482 - val_loss: 1.1022 - val_accuracy: 0.2895\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 0s 207us/sample - loss: 1.0864 - accuracy: 0.3482 - val_loss: 1.0974 - val_accuracy: 0.2895\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 0s 186us/sample - loss: 1.0811 - accuracy: 0.2143 - val_loss: 1.0942 - val_accuracy: 0.1579\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 0s 220us/sample - loss: 1.0788 - accuracy: 0.2054 - val_loss: 1.0919 - val_accuracy: 0.2105\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 0s 186us/sample - loss: 1.0737 - accuracy: 0.2857 - val_loss: 1.0892 - val_accuracy: 0.3158\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 0s 206us/sample - loss: 1.0712 - accuracy: 0.3393 - val_loss: 1.0874 - val_accuracy: 0.3158\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 0s 223us/sample - loss: 1.0690 - accuracy: 0.3393 - val_loss: 1.0850 - val_accuracy: 0.3158\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 0s 217us/sample - loss: 1.0666 - accuracy: 0.3393 - val_loss: 1.0837 - val_accuracy: 0.3158\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 0s 240us/sample - loss: 1.0658 - accuracy: 0.3393 - val_loss: 1.0820 - val_accuracy: 0.3158\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 0s 213us/sample - loss: 1.0640 - accuracy: 0.3393 - val_loss: 1.0810 - val_accuracy: 0.3158\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 0s 209us/sample - loss: 1.0618 - accuracy: 0.3393 - val_loss: 1.0797 - val_accuracy: 0.3158\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 0s 194us/sample - loss: 1.0613 - accuracy: 0.3393 - val_loss: 1.0790 - val_accuracy: 0.3158\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 0s 215us/sample - loss: 1.0590 - accuracy: 0.3393 - val_loss: 1.0778 - val_accuracy: 0.3158\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 0s 213us/sample - loss: 1.0574 - accuracy: 0.3393 - val_loss: 1.0763 - val_accuracy: 0.3158\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 0s 199us/sample - loss: 1.0563 - accuracy: 0.3393 - val_loss: 1.0748 - val_accuracy: 0.3158\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 0s 218us/sample - loss: 1.0533 - accuracy: 0.3393 - val_loss: 1.0733 - val_accuracy: 0.3158\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 0s 181us/sample - loss: 1.0516 - accuracy: 0.3393 - val_loss: 1.0722 - val_accuracy: 0.3158\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 0s 228us/sample - loss: 1.0500 - accuracy: 0.3393 - val_loss: 1.0707 - val_accuracy: 0.3158\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 0s 213us/sample - loss: 1.0484 - accuracy: 0.3393 - val_loss: 1.0693 - val_accuracy: 0.3158\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 0s 188us/sample - loss: 1.0463 - accuracy: 0.3393 - val_loss: 1.0678 - val_accuracy: 0.3158\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 0s 196us/sample - loss: 1.0447 - accuracy: 0.3393 - val_loss: 1.0663 - val_accuracy: 0.3158\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 0s 222us/sample - loss: 1.0432 - accuracy: 0.3393 - val_loss: 1.0645 - val_accuracy: 0.3158\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 0s 193us/sample - loss: 1.0411 - accuracy: 0.3393 - val_loss: 1.0625 - val_accuracy: 0.3158\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 0s 229us/sample - loss: 1.0396 - accuracy: 0.3393 - val_loss: 1.0603 - val_accuracy: 0.3158\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 0s 234us/sample - loss: 1.0384 - accuracy: 0.3393 - val_loss: 1.0588 - val_accuracy: 0.3158\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 0s 181us/sample - loss: 1.0366 - accuracy: 0.3393 - val_loss: 1.0577 - val_accuracy: 0.3158\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 0s 189us/sample - loss: 1.0343 - accuracy: 0.3393 - val_loss: 1.0560 - val_accuracy: 0.3158\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 0s 196us/sample - loss: 1.0317 - accuracy: 0.3393 - val_loss: 1.0536 - val_accuracy: 0.3158\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 0s 219us/sample - loss: 1.0299 - accuracy: 0.3393 - val_loss: 1.0512 - val_accuracy: 0.3158\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 0s 210us/sample - loss: 1.0291 - accuracy: 0.3393 - val_loss: 1.0483 - val_accuracy: 0.3158\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 0s 207us/sample - loss: 1.0260 - accuracy: 0.3393 - val_loss: 1.0463 - val_accuracy: 0.3158\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 0s 194us/sample - loss: 1.0237 - accuracy: 0.3393 - val_loss: 1.0444 - val_accuracy: 0.3158\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 0s 223us/sample - loss: 1.0211 - accuracy: 0.3393 - val_loss: 1.0417 - val_accuracy: 0.3158\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 0s 312us/sample - loss: 1.0186 - accuracy: 0.3393 - val_loss: 1.0384 - val_accuracy: 0.3158\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 0s 264us/sample - loss: 1.0169 - accuracy: 0.3393 - val_loss: 1.0352 - val_accuracy: 0.3158\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 0s 238us/sample - loss: 1.0139 - accuracy: 0.3393 - val_loss: 1.0329 - val_accuracy: 0.3158\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 0s 235us/sample - loss: 1.0124 - accuracy: 0.3393 - val_loss: 1.0299 - val_accuracy: 0.3158\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 0s 217us/sample - loss: 1.0086 - accuracy: 0.3393 - val_loss: 1.0267 - val_accuracy: 0.3158\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 0s 223us/sample - loss: 1.0054 - accuracy: 0.3393 - val_loss: 1.0239 - val_accuracy: 0.3158\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 0s 244us/sample - loss: 1.0026 - accuracy: 0.3393 - val_loss: 1.0212 - val_accuracy: 0.3158\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 0s 208us/sample - loss: 1.0017 - accuracy: 0.3393 - val_loss: 1.0179 - val_accuracy: 0.3158\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 0s 236us/sample - loss: 0.9979 - accuracy: 0.3393 - val_loss: 1.0141 - val_accuracy: 0.3158\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 0s 193us/sample - loss: 0.9939 - accuracy: 0.3393 - val_loss: 1.0114 - val_accuracy: 0.3158\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 0s 191us/sample - loss: 0.9902 - accuracy: 0.3393 - val_loss: 1.0074 - val_accuracy: 0.3158\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 0s 197us/sample - loss: 0.9875 - accuracy: 0.3393 - val_loss: 1.0034 - val_accuracy: 0.3158\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 0s 189us/sample - loss: 0.9841 - accuracy: 0.3393 - val_loss: 0.9998 - val_accuracy: 0.3158\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 0s 229us/sample - loss: 0.9809 - accuracy: 0.3393 - val_loss: 0.9958 - val_accuracy: 0.3158\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 0s 189us/sample - loss: 0.9763 - accuracy: 0.3393 - val_loss: 0.9909 - val_accuracy: 0.3158\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 0s 235us/sample - loss: 0.9724 - accuracy: 0.3393 - val_loss: 0.9859 - val_accuracy: 0.3158\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 0s 277us/sample - loss: 0.9683 - accuracy: 0.3482 - val_loss: 0.9817 - val_accuracy: 0.3158\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 0s 199us/sample - loss: 0.9644 - accuracy: 0.3482 - val_loss: 0.9775 - val_accuracy: 0.3158\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 0s 215us/sample - loss: 0.9607 - accuracy: 0.3482 - val_loss: 0.9725 - val_accuracy: 0.3158\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 0s 202us/sample - loss: 0.9556 - accuracy: 0.3482 - val_loss: 0.9670 - val_accuracy: 0.3158\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 0s 216us/sample - loss: 0.9516 - accuracy: 0.3482 - val_loss: 0.9611 - val_accuracy: 0.3421\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 0s 206us/sample - loss: 0.9468 - accuracy: 0.3750 - val_loss: 0.9562 - val_accuracy: 0.3684\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 0s 203us/sample - loss: 0.9432 - accuracy: 0.3750 - val_loss: 0.9505 - val_accuracy: 0.3947\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 0s 217us/sample - loss: 0.9365 - accuracy: 0.3839 - val_loss: 0.9449 - val_accuracy: 0.3947\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 0s 248us/sample - loss: 0.9321 - accuracy: 0.4018 - val_loss: 0.9391 - val_accuracy: 0.3947\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 0s 232us/sample - loss: 0.9259 - accuracy: 0.4554 - val_loss: 0.9328 - val_accuracy: 0.4211\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 0s 220us/sample - loss: 0.9202 - accuracy: 0.4643 - val_loss: 0.9251 - val_accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 0s 186us/sample - loss: 0.9142 - accuracy: 0.5446 - val_loss: 0.9183 - val_accuracy: 0.5263\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 0s 202us/sample - loss: 0.9081 - accuracy: 0.5625 - val_loss: 0.9110 - val_accuracy: 0.5789\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 0s 204us/sample - loss: 0.9023 - accuracy: 0.5625 - val_loss: 0.9031 - val_accuracy: 0.6316\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 0s 225us/sample - loss: 0.8956 - accuracy: 0.5893 - val_loss: 0.8954 - val_accuracy: 0.6579\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 0s 202us/sample - loss: 0.8891 - accuracy: 0.5982 - val_loss: 0.8877 - val_accuracy: 0.6842\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 0s 231us/sample - loss: 0.8823 - accuracy: 0.6071 - val_loss: 0.8801 - val_accuracy: 0.7105\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 0s 236us/sample - loss: 0.8755 - accuracy: 0.6339 - val_loss: 0.8736 - val_accuracy: 0.7105\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 0s 225us/sample - loss: 0.8700 - accuracy: 0.6339 - val_loss: 0.8665 - val_accuracy: 0.7105\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 0s 222us/sample - loss: 0.8636 - accuracy: 0.6339 - val_loss: 0.8614 - val_accuracy: 0.7105\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 0s 211us/sample - loss: 0.8580 - accuracy: 0.6339 - val_loss: 0.8554 - val_accuracy: 0.7105\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 0s 203us/sample - loss: 0.8530 - accuracy: 0.6339 - val_loss: 0.8498 - val_accuracy: 0.7105\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 0s 234us/sample - loss: 0.8485 - accuracy: 0.6339 - val_loss: 0.8452 - val_accuracy: 0.7105\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 0s 188us/sample - loss: 0.8436 - accuracy: 0.6339 - val_loss: 0.8404 - val_accuracy: 0.7105\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 0s 232us/sample - loss: 0.8382 - accuracy: 0.6339 - val_loss: 0.8351 - val_accuracy: 0.7105\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 0s 258us/sample - loss: 0.8337 - accuracy: 0.6339 - val_loss: 0.8297 - val_accuracy: 0.7105\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 0s 216us/sample - loss: 0.8297 - accuracy: 0.6339 - val_loss: 0.8254 - val_accuracy: 0.7105\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 0s 223us/sample - loss: 0.8247 - accuracy: 0.6339 - val_loss: 0.8216 - val_accuracy: 0.7105\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 0s 240us/sample - loss: 0.8200 - accuracy: 0.6339 - val_loss: 0.8173 - val_accuracy: 0.7105\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 0s 294us/sample - loss: 0.8159 - accuracy: 0.6339 - val_loss: 0.8131 - val_accuracy: 0.7105\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 0s 258us/sample - loss: 0.8121 - accuracy: 0.6339 - val_loss: 0.8091 - val_accuracy: 0.7105\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 0s 217us/sample - loss: 0.8076 - accuracy: 0.6339 - val_loss: 0.8053 - val_accuracy: 0.7105\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 0s 202us/sample - loss: 0.8035 - accuracy: 0.6339 - val_loss: 0.8006 - val_accuracy: 0.7105\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 0s 224us/sample - loss: 0.8011 - accuracy: 0.6339 - val_loss: 0.7967 - val_accuracy: 0.7105\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 0s 237us/sample - loss: 0.7963 - accuracy: 0.6339 - val_loss: 0.7937 - val_accuracy: 0.7105\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 0s 220us/sample - loss: 0.7932 - accuracy: 0.6339 - val_loss: 0.7903 - val_accuracy: 0.7105\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 0s 248us/sample - loss: 0.7892 - accuracy: 0.6339 - val_loss: 0.7851 - val_accuracy: 0.7105\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 0s 233us/sample - loss: 0.7863 - accuracy: 0.6339 - val_loss: 0.7811 - val_accuracy: 0.7105\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 0s 223us/sample - loss: 0.7830 - accuracy: 0.6339 - val_loss: 0.7774 - val_accuracy: 0.7105\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 0s 205us/sample - loss: 0.7788 - accuracy: 0.6339 - val_loss: 0.7734 - val_accuracy: 0.7105\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 0s 224us/sample - loss: 0.7761 - accuracy: 0.6339 - val_loss: 0.7694 - val_accuracy: 0.7105\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 0s 225us/sample - loss: 0.7722 - accuracy: 0.6429 - val_loss: 0.7656 - val_accuracy: 0.7105\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 0s 200us/sample - loss: 0.7679 - accuracy: 0.6429 - val_loss: 0.7620 - val_accuracy: 0.7105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f345f10afd0>"
      ]
     },
     "execution_count": 234,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "re9ItAR3yS3J"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liw0IFf9yVqH"
   },
   "source": [
    "### Make predictions\n",
    "- Predict labels on one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5238,
     "status": "ok",
     "timestamp": 1580042218346,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "H5sBybi6mlLl",
    "outputId": "86287a53-4ec8-404a-9633-860e049e9516"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10825843, 0.38305363, 0.50868785]], dtype=float32)"
      ]
     },
     "execution_count": 235,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSUgMq3m0bG7"
   },
   "source": [
    "### Compare the prediction with actual label\n",
    "- Print the same row as done in the previous step but of actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5224,
     "status": "ok",
     "timestamp": 1580042218347,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "K5WbwVPyz-qQ",
    "outputId": "c57e5b91-14ae-4fa3-c7c9-85b56725f124"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0]])"
      ]
     },
     "execution_count": 236,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5212,
     "status": "ok",
     "timestamp": 1580042218348,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "hDBVh2uaw_0H",
    "outputId": "279dd262-01a0-455a-f18c-923cb16325a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 148us/sample - loss: 0.7620 - accuracy: 0.7105\n",
      "[0.7619709843083432, 0.7105263]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrTKwbgE7NFT"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1UBYPNp5Tn1"
   },
   "source": [
    "# Stock prices dataset\n",
    "The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n",
    "\n",
    "## Description\n",
    "A brief description of columns.\n",
    "- open: The opening market price of the equity symbol on the date\n",
    "- high: The highest market price of the equity symbol on the date\n",
    "- low: The lowest recorded market price of the equity symbol on the date\n",
    "- close: The closing recorded price of the equity symbol on the date\n",
    "- symbol: Symbol of the listed company\n",
    "- volume: Total traded volume of the equity symbol on the date\n",
    "- date: Date of record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctH_ZW5g-M3g"
   },
   "source": [
    "### Specifying the TensorFlow version\n",
    "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQbdODpH-M3r"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFQWH1tj-M38"
   },
   "source": [
    "### Import TensorFlow\n",
    "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5195,
     "status": "ok",
     "timestamp": 1580042218349,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "Ho5n-xhd-M3_",
    "outputId": "8e59ce16-d051-4633-c5a7-3e6ea0ca0f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgkl0qu6-M4F"
   },
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKgTyuA3-M4G"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_88voqAH-O6J"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRHCeJqP-evf"
   },
   "source": [
    "### Load the data\n",
    "- load the csv file and read it using pandas\n",
    "- file name is prices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gDC6cSW_FSK"
   },
   "outputs": [],
   "source": [
    "# Code to read csv file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11487,
     "status": "ok",
     "timestamp": 1580042224668,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "STCAvSVgRS9c",
    "outputId": "db9351db-89d0-4cfa-e570-26e1529dc337"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.510002</td>\n",
       "      <td>115.550003</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>116.059998</td>\n",
       "      <td>1098000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-01-13 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.459999</td>\n",
       "      <td>112.849998</td>\n",
       "      <td>112.589996</td>\n",
       "      <td>117.070000</td>\n",
       "      <td>949600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-01-14 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.510002</td>\n",
       "      <td>114.379997</td>\n",
       "      <td>110.050003</td>\n",
       "      <td>115.029999</td>\n",
       "      <td>785300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-01-15 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.330002</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>111.919998</td>\n",
       "      <td>114.879997</td>\n",
       "      <td>1093700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-01-19 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.660004</td>\n",
       "      <td>110.379997</td>\n",
       "      <td>109.870003</td>\n",
       "      <td>115.870003</td>\n",
       "      <td>1523500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-01-20 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>109.059998</td>\n",
       "      <td>109.300003</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>111.599998</td>\n",
       "      <td>1653900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-01-21 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>109.730003</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>110.580002</td>\n",
       "      <td>944300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-01-22 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>111.879997</td>\n",
       "      <td>111.949997</td>\n",
       "      <td>110.190002</td>\n",
       "      <td>112.949997</td>\n",
       "      <td>744900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>111.320000</td>\n",
       "      <td>110.120003</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>114.629997</td>\n",
       "      <td>703800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>110.419998</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>107.300003</td>\n",
       "      <td>111.400002</td>\n",
       "      <td>563100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-01-27 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>110.769997</td>\n",
       "      <td>110.709999</td>\n",
       "      <td>109.019997</td>\n",
       "      <td>112.570000</td>\n",
       "      <td>896100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-01-28 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>110.900002</td>\n",
       "      <td>112.580002</td>\n",
       "      <td>109.900002</td>\n",
       "      <td>112.970001</td>\n",
       "      <td>680400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-01-29 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.349998</td>\n",
       "      <td>114.470001</td>\n",
       "      <td>111.669998</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>749900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-02-01 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>112.900002</td>\n",
       "      <td>114.849998</td>\n",
       "      <td>574200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-02-02 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.250000</td>\n",
       "      <td>110.559998</td>\n",
       "      <td>109.750000</td>\n",
       "      <td>113.860001</td>\n",
       "      <td>694800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-02-03 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.379997</td>\n",
       "      <td>114.050003</td>\n",
       "      <td>109.639999</td>\n",
       "      <td>114.639999</td>\n",
       "      <td>896300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-02-04 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>115.709999</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>116.320000</td>\n",
       "      <td>956300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-02-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.120003</td>\n",
       "      <td>114.019997</td>\n",
       "      <td>109.709999</td>\n",
       "      <td>116.489998</td>\n",
       "      <td>997100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-02-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.300003</td>\n",
       "      <td>111.160004</td>\n",
       "      <td>110.459999</td>\n",
       "      <td>113.300003</td>\n",
       "      <td>1200500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-02-09 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>111.169998</td>\n",
       "      <td>110.650002</td>\n",
       "      <td>109.639999</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>1725200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-02-10 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>106.730003</td>\n",
       "      <td>107.519997</td>\n",
       "      <td>106.360001</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>1946000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016-02-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>105.629997</td>\n",
       "      <td>107.129997</td>\n",
       "      <td>104.110001</td>\n",
       "      <td>109.260002</td>\n",
       "      <td>1319500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2016-02-12 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>108.559998</td>\n",
       "      <td>107.839996</td>\n",
       "      <td>107.070000</td>\n",
       "      <td>109.430000</td>\n",
       "      <td>922400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016-02-16 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>109.110001</td>\n",
       "      <td>110.769997</td>\n",
       "      <td>107.010002</td>\n",
       "      <td>111.300003</td>\n",
       "      <td>1185100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016-02-17 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>110.830002</td>\n",
       "      <td>111.239998</td>\n",
       "      <td>107.970001</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>921500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851234</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WAT</td>\n",
       "      <td>135.240005</td>\n",
       "      <td>134.389999</td>\n",
       "      <td>133.710007</td>\n",
       "      <td>135.300003</td>\n",
       "      <td>464200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851235</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WBA</td>\n",
       "      <td>83.459999</td>\n",
       "      <td>82.760002</td>\n",
       "      <td>82.419998</td>\n",
       "      <td>83.620003</td>\n",
       "      <td>3343200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851236</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WDC</td>\n",
       "      <td>68.550003</td>\n",
       "      <td>67.949997</td>\n",
       "      <td>67.610001</td>\n",
       "      <td>69.400002</td>\n",
       "      <td>2824100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851237</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WEC</td>\n",
       "      <td>58.980000</td>\n",
       "      <td>58.650002</td>\n",
       "      <td>58.419998</td>\n",
       "      <td>59.119999</td>\n",
       "      <td>1221800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851238</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WFC</td>\n",
       "      <td>54.889999</td>\n",
       "      <td>55.110001</td>\n",
       "      <td>54.790001</td>\n",
       "      <td>55.360001</td>\n",
       "      <td>15095500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851239</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WFM</td>\n",
       "      <td>31.059999</td>\n",
       "      <td>30.760000</td>\n",
       "      <td>30.670000</td>\n",
       "      <td>31.299999</td>\n",
       "      <td>2707500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851240</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WHR</td>\n",
       "      <td>183.800003</td>\n",
       "      <td>181.770004</td>\n",
       "      <td>180.869995</td>\n",
       "      <td>184.289993</td>\n",
       "      <td>458200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851241</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WM</td>\n",
       "      <td>71.269997</td>\n",
       "      <td>70.910004</td>\n",
       "      <td>70.750000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>1230600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851242</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WMB</td>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.139999</td>\n",
       "      <td>30.889999</td>\n",
       "      <td>31.650000</td>\n",
       "      <td>3980300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851243</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WMT</td>\n",
       "      <td>69.120003</td>\n",
       "      <td>69.120003</td>\n",
       "      <td>68.830002</td>\n",
       "      <td>69.430000</td>\n",
       "      <td>6872000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851244</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WRK</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>50.529999</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>811200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851245</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WU</td>\n",
       "      <td>21.840000</td>\n",
       "      <td>21.719999</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>2538900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851246</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WY</td>\n",
       "      <td>30.450001</td>\n",
       "      <td>30.090000</td>\n",
       "      <td>29.950001</td>\n",
       "      <td>30.450001</td>\n",
       "      <td>2825300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851247</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WYN</td>\n",
       "      <td>76.849998</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>76.180000</td>\n",
       "      <td>76.970001</td>\n",
       "      <td>524600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851248</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WYNN</td>\n",
       "      <td>87.099998</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>85.570000</td>\n",
       "      <td>87.449997</td>\n",
       "      <td>1888500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851249</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XEC</td>\n",
       "      <td>136.520004</td>\n",
       "      <td>135.899994</td>\n",
       "      <td>135.309998</td>\n",
       "      <td>137.559998</td>\n",
       "      <td>466100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851250</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XEL</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.700001</td>\n",
       "      <td>40.560001</td>\n",
       "      <td>41.070000</td>\n",
       "      <td>1887600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851251</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XL</td>\n",
       "      <td>37.360001</td>\n",
       "      <td>37.259998</td>\n",
       "      <td>37.060001</td>\n",
       "      <td>37.419998</td>\n",
       "      <td>959200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851252</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XLNX</td>\n",
       "      <td>61.090000</td>\n",
       "      <td>60.369999</td>\n",
       "      <td>60.020000</td>\n",
       "      <td>61.480000</td>\n",
       "      <td>2111700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851253</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>90.029999</td>\n",
       "      <td>90.260002</td>\n",
       "      <td>90.010002</td>\n",
       "      <td>90.699997</td>\n",
       "      <td>9117800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851254</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XRAY</td>\n",
       "      <td>58.290001</td>\n",
       "      <td>57.730000</td>\n",
       "      <td>57.540001</td>\n",
       "      <td>58.360001</td>\n",
       "      <td>949200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851255</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XRX</td>\n",
       "      <td>8.720000</td>\n",
       "      <td>8.730000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>11250400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851256</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XYL</td>\n",
       "      <td>49.980000</td>\n",
       "      <td>49.520000</td>\n",
       "      <td>49.360001</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>646200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851257</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>YHOO</td>\n",
       "      <td>38.720001</td>\n",
       "      <td>38.669998</td>\n",
       "      <td>38.430000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>6431600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851258</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>YUM</td>\n",
       "      <td>63.930000</td>\n",
       "      <td>63.330002</td>\n",
       "      <td>63.160000</td>\n",
       "      <td>63.939999</td>\n",
       "      <td>1887100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851259</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZBH</td>\n",
       "      <td>103.309998</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>102.849998</td>\n",
       "      <td>103.930000</td>\n",
       "      <td>973800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851260</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>43.070000</td>\n",
       "      <td>43.040001</td>\n",
       "      <td>42.689999</td>\n",
       "      <td>43.310001</td>\n",
       "      <td>1938100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851261</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>53.639999</td>\n",
       "      <td>53.529999</td>\n",
       "      <td>53.270000</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>1701200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851262</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>AIV</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>45.450001</td>\n",
       "      <td>44.410000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>1380900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851263</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>FTV</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>53.630001</td>\n",
       "      <td>53.389999</td>\n",
       "      <td>54.480000</td>\n",
       "      <td>705100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>851264 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date symbol        open       close         low  \\\n",
       "0       2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998   \n",
       "1       2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002   \n",
       "2       2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000   \n",
       "3       2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000   \n",
       "4       2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996   \n",
       "5       2016-01-12 00:00:00   WLTW  115.510002  115.550003  114.500000   \n",
       "6       2016-01-13 00:00:00   WLTW  116.459999  112.849998  112.589996   \n",
       "7       2016-01-14 00:00:00   WLTW  113.510002  114.379997  110.050003   \n",
       "8       2016-01-15 00:00:00   WLTW  113.330002  112.529999  111.919998   \n",
       "9       2016-01-19 00:00:00   WLTW  113.660004  110.379997  109.870003   \n",
       "10      2016-01-20 00:00:00   WLTW  109.059998  109.300003  108.320000   \n",
       "11      2016-01-21 00:00:00   WLTW  109.730003  110.000000  108.320000   \n",
       "12      2016-01-22 00:00:00   WLTW  111.879997  111.949997  110.190002   \n",
       "13      2016-01-25 00:00:00   WLTW  111.320000  110.120003  110.000000   \n",
       "14      2016-01-26 00:00:00   WLTW  110.419998  111.000000  107.300003   \n",
       "15      2016-01-27 00:00:00   WLTW  110.769997  110.709999  109.019997   \n",
       "16      2016-01-28 00:00:00   WLTW  110.900002  112.580002  109.900002   \n",
       "17      2016-01-29 00:00:00   WLTW  113.349998  114.470001  111.669998   \n",
       "18      2016-02-01 00:00:00   WLTW  114.000000  114.500000  112.900002   \n",
       "19      2016-02-02 00:00:00   WLTW  113.250000  110.559998  109.750000   \n",
       "20      2016-02-03 00:00:00   WLTW  113.379997  114.050003  109.639999   \n",
       "21      2016-02-04 00:00:00   WLTW  114.080002  115.709999  114.080002   \n",
       "22      2016-02-05 00:00:00   WLTW  115.120003  114.019997  109.709999   \n",
       "23      2016-02-08 00:00:00   WLTW  113.300003  111.160004  110.459999   \n",
       "24      2016-02-09 00:00:00   WLTW  111.169998  110.650002  109.639999   \n",
       "25      2016-02-10 00:00:00   WLTW  106.730003  107.519997  106.360001   \n",
       "26      2016-02-11 00:00:00   WLTW  105.629997  107.129997  104.110001   \n",
       "27      2016-02-12 00:00:00   WLTW  108.559998  107.839996  107.070000   \n",
       "28      2016-02-16 00:00:00   WLTW  109.110001  110.769997  107.010002   \n",
       "29      2016-02-17 00:00:00   WLTW  110.830002  111.239998  107.970001   \n",
       "...                     ...    ...         ...         ...         ...   \n",
       "851234           2016-12-30    WAT  135.240005  134.389999  133.710007   \n",
       "851235           2016-12-30    WBA   83.459999   82.760002   82.419998   \n",
       "851236           2016-12-30    WDC   68.550003   67.949997   67.610001   \n",
       "851237           2016-12-30    WEC   58.980000   58.650002   58.419998   \n",
       "851238           2016-12-30    WFC   54.889999   55.110001   54.790001   \n",
       "851239           2016-12-30    WFM   31.059999   30.760000   30.670000   \n",
       "851240           2016-12-30    WHR  183.800003  181.770004  180.869995   \n",
       "851241           2016-12-30     WM   71.269997   70.910004   70.750000   \n",
       "851242           2016-12-30    WMB   30.940001   31.139999   30.889999   \n",
       "851243           2016-12-30    WMT   69.120003   69.120003   68.830002   \n",
       "851244           2016-12-30    WRK   51.840000   50.770000   50.529999   \n",
       "851245           2016-12-30     WU   21.840000   21.719999   21.600000   \n",
       "851246           2016-12-30     WY   30.450001   30.090000   29.950001   \n",
       "851247           2016-12-30    WYN   76.849998   76.370003   76.180000   \n",
       "851248           2016-12-30   WYNN   87.099998   86.510002   85.570000   \n",
       "851249           2016-12-30    XEC  136.520004  135.899994  135.309998   \n",
       "851250           2016-12-30    XEL   41.000000   40.700001   40.560001   \n",
       "851251           2016-12-30     XL   37.360001   37.259998   37.060001   \n",
       "851252           2016-12-30   XLNX   61.090000   60.369999   60.020000   \n",
       "851253           2016-12-30    XOM   90.029999   90.260002   90.010002   \n",
       "851254           2016-12-30   XRAY   58.290001   57.730000   57.540001   \n",
       "851255           2016-12-30    XRX    8.720000    8.730000    8.700000   \n",
       "851256           2016-12-30    XYL   49.980000   49.520000   49.360001   \n",
       "851257           2016-12-30   YHOO   38.720001   38.669998   38.430000   \n",
       "851258           2016-12-30    YUM   63.930000   63.330002   63.160000   \n",
       "851259           2016-12-30    ZBH  103.309998  103.199997  102.849998   \n",
       "851260           2016-12-30   ZION   43.070000   43.040001   42.689999   \n",
       "851261           2016-12-30    ZTS   53.639999   53.529999   53.270000   \n",
       "851262  2016-12-30 00:00:00    AIV   44.730000   45.450001   44.410000   \n",
       "851263  2016-12-30 00:00:00    FTV   54.200001   53.630001   53.389999   \n",
       "\n",
       "              high      volume  \n",
       "0       126.250000   2163600.0  \n",
       "1       125.540001   2386400.0  \n",
       "2       119.739998   2489500.0  \n",
       "3       117.440002   2006300.0  \n",
       "4       117.330002   1408600.0  \n",
       "5       116.059998   1098000.0  \n",
       "6       117.070000    949600.0  \n",
       "7       115.029999    785300.0  \n",
       "8       114.879997   1093700.0  \n",
       "9       115.870003   1523500.0  \n",
       "10      111.599998   1653900.0  \n",
       "11      110.580002    944300.0  \n",
       "12      112.949997    744900.0  \n",
       "13      114.629997    703800.0  \n",
       "14      111.400002    563100.0  \n",
       "15      112.570000    896100.0  \n",
       "16      112.970001    680400.0  \n",
       "17      114.589996    749900.0  \n",
       "18      114.849998    574200.0  \n",
       "19      113.860001    694800.0  \n",
       "20      114.639999    896300.0  \n",
       "21      116.320000    956300.0  \n",
       "22      116.489998    997100.0  \n",
       "23      113.300003   1200500.0  \n",
       "24      112.110001   1725200.0  \n",
       "25      112.110001   1946000.0  \n",
       "26      109.260002   1319500.0  \n",
       "27      109.430000    922400.0  \n",
       "28      111.300003   1185100.0  \n",
       "29      112.110001    921500.0  \n",
       "...            ...         ...  \n",
       "851234  135.300003    464200.0  \n",
       "851235   83.620003   3343200.0  \n",
       "851236   69.400002   2824100.0  \n",
       "851237   59.119999   1221800.0  \n",
       "851238   55.360001  15095500.0  \n",
       "851239   31.299999   2707500.0  \n",
       "851240  184.289993    458200.0  \n",
       "851241   71.500000   1230600.0  \n",
       "851242   31.650000   3980300.0  \n",
       "851243   69.430000   6872000.0  \n",
       "851244   51.840000    811200.0  \n",
       "851245   21.900000   2538900.0  \n",
       "851246   30.450001   2825300.0  \n",
       "851247   76.970001    524600.0  \n",
       "851248   87.449997   1888500.0  \n",
       "851249  137.559998    466100.0  \n",
       "851250   41.070000   1887600.0  \n",
       "851251   37.419998    959200.0  \n",
       "851252   61.480000   2111700.0  \n",
       "851253   90.699997   9117800.0  \n",
       "851254   58.360001    949200.0  \n",
       "851255    8.800000  11250400.0  \n",
       "851256   50.000000    646200.0  \n",
       "851257   39.000000   6431600.0  \n",
       "851258   63.939999   1887100.0  \n",
       "851259  103.930000    973800.0  \n",
       "851260   43.310001   1938100.0  \n",
       "851261   53.740002   1701200.0  \n",
       "851262   45.590000   1380900.0  \n",
       "851263   54.480000    705100.0  \n",
       "\n",
       "[851264 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "link = \"https://drive.google.com/open?id=1xoN6aDneJAsEAHAY-sC_ThGvFaUe7_Qf\"\n",
    "fluff, id = link.split('=')\n",
    "print (id) \n",
    "\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('prices.csv')  \n",
    "df = pd.read_csv('prices.csv')\n",
    "# Dataset is now stored in a Pandas Dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlLKVPVH_BCT"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J4BlzVA_gZd"
   },
   "source": [
    "### Drop columnns\n",
    "- drop \"date\" and \"symbol\" column from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11473,
     "status": "ok",
     "timestamp": 1580042224669,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "IKEK8aEE_Csx",
    "outputId": "623a6a37-675c-4e99-87ea-47e5444eafa2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = df.drop(['date','symbol'], axis=1)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTPhO6v-AiZt"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsZXmF3NAkna"
   },
   "source": [
    "### Take initial rows\n",
    "- Take first 1000 rows from the data\n",
    "- This step is done to make the execution faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11454,
     "status": "ok",
     "timestamp": 1580042224670,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "aKs04iIHAjxN",
    "outputId": "6e232305-e308-446a-96f4-9876c07e50d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(851264, 5)\n",
      "(1000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_1.shape)\n",
    "stock_df = df_1[0:1000]\n",
    "print(stock_df.shape)\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vGtnapgBIJm"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8u_jlbABTip"
   },
   "source": [
    "### Get features and label from the dataset in separate variable\n",
    "- Take \"open\", \"close\", \"low\", \"high\" columns as features\n",
    "- Take \"volume\" column as label\n",
    "- Normalize label column by dividing it with 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11442,
     "status": "ok",
     "timestamp": 1580042224670,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "xQjCMzUXBJbg",
    "outputId": "82c7e24d-ab29-4a14-c74b-28e093b2f4ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "           open       close         low        high\n",
      "0    123.430000  125.839996  122.309998  126.250000\n",
      "1    125.239998  119.980003  119.940002  125.540001\n",
      "2    116.379997  114.949997  114.930000  119.739998\n",
      "3    115.480003  116.620003  113.500000  117.440002\n",
      "4    117.010002  114.970001  114.089996  117.330002\n",
      "5    115.510002  115.550003  114.500000  116.059998\n",
      "6    116.459999  112.849998  112.589996  117.070000\n",
      "7    113.510002  114.379997  110.050003  115.029999\n",
      "8    113.330002  112.529999  111.919998  114.879997\n",
      "9    113.660004  110.379997  109.870003  115.870003\n",
      "10   109.059998  109.300003  108.320000  111.599998\n",
      "11   109.730003  110.000000  108.320000  110.580002\n",
      "12   111.879997  111.949997  110.190002  112.949997\n",
      "13   111.320000  110.120003  110.000000  114.629997\n",
      "14   110.419998  111.000000  107.300003  111.400002\n",
      "15   110.769997  110.709999  109.019997  112.570000\n",
      "16   110.900002  112.580002  109.900002  112.970001\n",
      "17   113.349998  114.470001  111.669998  114.589996\n",
      "18   114.000000  114.500000  112.900002  114.849998\n",
      "19   113.250000  110.559998  109.750000  113.860001\n",
      "20   113.379997  114.050003  109.639999  114.639999\n",
      "21   114.080002  115.709999  114.080002  116.320000\n",
      "22   115.120003  114.019997  109.709999  116.489998\n",
      "23   113.300003  111.160004  110.459999  113.300003\n",
      "24   111.169998  110.650002  109.639999  112.110001\n",
      "25   106.730003  107.519997  106.360001  112.110001\n",
      "26   105.629997  107.129997  104.110001  109.260002\n",
      "27   108.559998  107.839996  107.070000  109.430000\n",
      "28   109.110001  110.769997  107.010002  111.300003\n",
      "29   110.830002  111.239998  107.970001  112.110001\n",
      "..          ...         ...         ...         ...\n",
      "970   19.240000   18.680000   18.530001   19.430000\n",
      "971   20.520000   20.290001   19.709999   20.549999\n",
      "972   12.950000   13.550000   12.720000   13.600000\n",
      "973   76.059998   75.440002   75.349998   76.370003\n",
      "974   19.620001   19.830000   19.480000   19.830000\n",
      "975   88.000000   87.370003   87.139999   88.239998\n",
      "976   31.010000   30.959999   30.700001   31.120001\n",
      "977   35.900002   35.189999   34.930000   35.910000\n",
      "978   76.620003   77.650002   76.550003   77.790001\n",
      "979   25.799999   26.420000   25.719999   26.610001\n",
      "980   30.370001   31.059999   30.309999   31.110001\n",
      "981   23.120001   22.920000   22.740000   23.129999\n",
      "982   39.790001   39.610001   39.090000   39.799999\n",
      "983   24.669997   25.140004   24.669997   25.160000\n",
      "984   11.250000   11.770000   11.200000   11.790000\n",
      "985    1.630000    1.650000    1.600000    1.660000\n",
      "986   17.020000   16.860001   16.790001   17.209999\n",
      "987  257.840004  256.079998  253.050003  257.959995\n",
      "988   48.099998   48.150002   47.700001   48.369999\n",
      "989   34.529999   34.430000   34.080002   34.750000\n",
      "990   27.570000   27.790001   27.370000   27.919999\n",
      "991   14.200000   14.420000   14.120000   14.430000\n",
      "992   19.900000   19.580000   19.330000   20.059999\n",
      "993   62.660000   62.299999   62.189999   62.750000\n",
      "994   29.219999   28.719999   28.629999   29.309999\n",
      "995   63.310001   63.590000   63.240002   63.639999\n",
      "996   27.160000   26.990000   26.680000   27.299999\n",
      "997   28.320000   28.770000   28.010000   28.809999\n",
      "998   44.000000   44.799999   43.750000   44.810001\n",
      "999   36.080002   37.139999   36.009998   37.230000\n",
      "\n",
      "[1000 rows x 4 columns]\n",
      "0       2.1636\n",
      "1       2.3864\n",
      "2       2.4895\n",
      "3       2.0063\n",
      "4       1.4086\n",
      "5       1.0980\n",
      "6       0.9496\n",
      "7       0.7853\n",
      "8       1.0937\n",
      "9       1.5235\n",
      "10      1.6539\n",
      "11      0.9443\n",
      "12      0.7449\n",
      "13      0.7038\n",
      "14      0.5631\n",
      "15      0.8961\n",
      "16      0.6804\n",
      "17      0.7499\n",
      "18      0.5742\n",
      "19      0.6948\n",
      "20      0.8963\n",
      "21      0.9563\n",
      "22      0.9971\n",
      "23      1.2005\n",
      "24      1.7252\n",
      "25      1.9460\n",
      "26      1.3195\n",
      "27      0.9224\n",
      "28      1.1851\n",
      "29      0.9215\n",
      "        ...   \n",
      "970     9.0975\n",
      "971     2.2421\n",
      "972     6.2051\n",
      "973     0.8646\n",
      "974     0.7092\n",
      "975     1.1296\n",
      "976     3.4493\n",
      "977     7.5171\n",
      "978     2.3565\n",
      "979     4.8392\n",
      "980     3.6846\n",
      "981    14.4284\n",
      "982     1.4633\n",
      "983     0.7496\n",
      "984    13.3551\n",
      "985    11.5383\n",
      "986     9.9313\n",
      "987    12.9060\n",
      "988     0.3699\n",
      "989     2.7010\n",
      "990     2.6278\n",
      "991     3.2587\n",
      "992     5.2061\n",
      "993     7.0990\n",
      "994     7.7955\n",
      "995     2.1332\n",
      "996     1.9824\n",
      "997    37.1528\n",
      "998     6.5686\n",
      "999     5.6043\n",
      "Name: volume, Length: 1000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    " X = stock_df.iloc[:,0:4]\n",
    " Y = stock_df.iloc[:,4:]\n",
    " Y = Y['volume'].div(1000000)\n",
    " print(type(X))\n",
    " print(type(Y))\n",
    " print(X) \n",
    " print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTAKzlxZBz0z"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfY8Km1Zzyt2"
   },
   "source": [
    "### Convert data\n",
    "- Convert features and labels to numpy array\n",
    "- Convert their data type to \"float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11424,
     "status": "ok",
     "timestamp": 1580042224671,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "Ko7nnQVbYENh",
    "outputId": "6ceb7cd8-941d-4718-8377-4e7651420efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "type: <class 'numpy.ndarray'> [[123.43 125.84 122.31 126.25]\n",
      " [125.24 119.98 119.94 125.54]\n",
      " [116.38 114.95 114.93 119.74]\n",
      " ...\n",
      " [ 28.32  28.77  28.01  28.81]\n",
      " [ 44.    44.8   43.75  44.81]\n",
      " [ 36.08  37.14  36.01  37.23]]\n",
      "data_type: float32 [2.163600e+00 2.386400e+00 2.489500e+00 2.006300e+00 1.408600e+00\n",
      " 1.098000e+00 9.496000e-01 7.853000e-01 1.093700e+00 1.523500e+00\n",
      " 1.653900e+00 9.443000e-01 7.449000e-01 7.038000e-01 5.631000e-01\n",
      " 8.961000e-01 6.804000e-01 7.499000e-01 5.742000e-01 6.948000e-01\n",
      " 8.963000e-01 9.563000e-01 9.971000e-01 1.200500e+00 1.725200e+00\n",
      " 1.946000e+00 1.319500e+00 9.224000e-01 1.185100e+00 9.215000e-01\n",
      " 4.409000e-01 1.244300e+00 6.813000e-01 4.112000e-01 4.473000e-01\n",
      " 5.592000e-01 4.599000e-01 9.716000e-01 6.941000e-01 1.159600e+00\n",
      " 8.847000e-01 6.920000e-01 8.980000e-01 8.445000e-01 7.965000e-01\n",
      " 4.168000e-01 5.902000e-01 4.951000e-01 5.158000e-01 5.621000e-01\n",
      " 8.093000e-01 1.607300e+00 1.001300e+00 7.114000e-01 4.442000e-01\n",
      " 5.341000e-01 7.982000e-01 5.910000e-01 4.432000e-01 5.780000e-01\n",
      " 7.651000e-01 5.143000e-01 1.150000e+00 2.035300e+00 7.022000e-01\n",
      " 6.891000e-01 5.532000e-01 7.759000e-01 6.217000e-01 3.581000e-01\n",
      " 9.393000e-01 7.355000e-01 9.246000e-01 6.588000e-01 1.055400e+00\n",
      " 6.953000e-01 4.600000e-01 5.329000e-01 4.626000e-01 9.107000e-01\n",
      " 8.896000e-01 7.347000e-01 1.304800e+00 1.597300e+00 1.917500e+00\n",
      " 3.221500e+00 9.198000e-01 1.991600e+00 5.948000e-01 4.758000e-01\n",
      " 7.151000e-01 5.285000e-01 5.800000e-01 6.297000e-01 5.982000e-01\n",
      " 8.167000e-01 6.408000e-01 7.611000e-01 4.621000e-01 6.270000e-01\n",
      " 7.225000e-01 1.384200e+00 4.415000e-01 4.236000e-01 4.967000e-01\n",
      " 5.367000e-01 4.294000e-01 7.191000e-01 3.945000e-01 5.480000e-01\n",
      " 2.633000e-01 3.293000e-01 3.787000e-01 4.048000e-01 6.593000e-01\n",
      " 5.503000e-01 4.856000e-01 5.619000e-01 5.123000e-01 1.638000e+00\n",
      " 8.844000e-01 7.952000e-01 1.049700e+00 1.162100e+00 8.584000e-01\n",
      " 7.614000e-01 7.499000e-01 5.911000e-01 6.928000e-01 4.826000e-01\n",
      " 8.440000e-01 6.300000e-01 5.671000e-01 5.122000e-01 4.622000e-01\n",
      " 7.061000e-01 5.339000e-01 3.747000e-01 3.738000e-01 3.657000e-01\n",
      " 3.139000e-01 4.010000e-01 2.755000e-01 4.267000e-01 5.746000e-01\n",
      " 7.932000e-01 4.469000e-01 7.667000e-01 1.865200e+00 9.308000e-01\n",
      " 5.027000e-01 7.097000e-01 5.639000e-01 3.429000e-01 4.788000e-01\n",
      " 5.018000e-01 4.103000e-01 4.438000e-01 1.414800e+00 5.707000e-01\n",
      " 3.913000e-01 4.252000e-01 5.263000e-01 5.147000e-01 4.565000e-01\n",
      " 4.141000e-01 4.645000e-01 5.262000e-01 4.873000e-01 7.523000e-01\n",
      " 6.325000e-01 7.283000e-01 6.411000e-01 6.555000e-01 4.889000e-01\n",
      " 7.419000e-01 7.134000e-01 1.847200e+00 3.107000e-01 3.110000e-01\n",
      " 9.277000e-01 7.165000e-01 7.970000e-01 4.103000e-01 5.036000e-01\n",
      " 5.672000e-01 1.504500e+00 1.461900e+00 1.142100e+00 1.068500e+00\n",
      " 7.002000e-01 7.672000e-01 8.551000e-01 4.238000e-01 5.757000e-01\n",
      " 6.819000e-01 5.403000e-01 5.480000e-01 5.645000e-01 5.056000e-01\n",
      " 4.356000e-01 5.548000e-01 4.966000e-01 3.120000e-01 4.147000e-01\n",
      " 4.783000e-01 4.170000e-01 4.457000e-01 4.592000e-01 4.758000e-01\n",
      " 6.932000e-01 7.367000e-01 3.531200e+00 1.345600e+00 9.676000e-01\n",
      " 1.291600e+00 1.524500e+00 7.543000e-01 1.089100e+00 6.397000e-01\n",
      " 8.184000e-01 5.245000e-01 8.190000e-01 4.759000e-01 1.011700e+00\n",
      " 3.802000e-01 2.840000e-01 4.399000e-01 3.529000e-01 6.767000e-01\n",
      " 8.987000e-01 8.558000e-01 6.047000e-01 1.301200e+00 9.283000e-01\n",
      " 9.885000e-01 9.434000e-01 8.225000e-01 1.168200e+00 8.464000e-01\n",
      " 8.266000e-01 1.232700e+00 9.376000e-01 6.515000e-01 6.146000e-01\n",
      " 5.572000e-01 3.619000e-01 3.829000e-01 4.299000e-01 2.166000e-01\n",
      " 4.664000e-01 3.815500e+00 9.837300e+00 1.701700e+00 1.234324e+02\n",
      " 2.455900e+00 1.082900e+01 3.650100e+00 4.710200e+00 2.102700e+00\n",
      " 3.472500e+00 3.930100e+00 7.943000e-01 2.228600e+00 1.299300e+00\n",
      " 4.076600e+00 4.597600e+00 5.671300e+00 2.362000e+00 8.564000e-01\n",
      " 7.750900e+00 1.228200e+00 3.793000e-01 3.015600e+00 7.129000e-01\n",
      " 1.802400e+00 2.631000e+00 1.128200e+00 1.861510e+01 8.767000e-01\n",
      " 3.061000e-01 5.277400e+00 2.238700e+00 2.750500e+00 7.599900e+00\n",
      " 1.650400e+00 4.641800e+00 3.407100e+00 2.364900e+00 3.324500e+00\n",
      " 1.131400e+00 2.457200e+00 1.151210e+01 9.306600e+00 1.483400e+00\n",
      " 5.387000e-01 1.301200e+00 2.176100e+00 6.894300e+00 8.292000e-01\n",
      " 4.083000e-01 6.186700e+00 1.808452e+02 1.146750e+01 2.971500e+00\n",
      " 4.550800e+00 6.433800e+00 1.371800e+00 1.793200e+00 3.746700e+00\n",
      " 5.888000e+00 2.469700e+00 6.127700e+00 2.387000e-01 1.234200e+00\n",
      " 1.437610e+01 1.433230e+01 1.286000e+00 1.511500e+00 4.067930e+01\n",
      " 3.385100e+00 4.009700e+00 3.824400e+00 7.325600e+00 2.670300e+00\n",
      " 4.553000e+00 6.710900e+00 1.964100e+00 5.372700e+00 4.832000e+00\n",
      " 3.055200e+00 6.172000e+00 2.232400e+00 3.114680e+01 8.229000e-01\n",
      " 3.227300e+00 1.261400e+00 4.874200e+00 1.014300e+00 1.438400e+00\n",
      " 1.357340e+01 2.660500e+00 3.139000e-01 1.148600e+00 5.875500e+00\n",
      " 6.600000e-01 2.239700e+00 4.447400e+00 4.514800e+00 3.350600e+00\n",
      " 8.281000e-01 4.812000e-01 1.388080e+01 3.280200e+00 1.574200e+00\n",
      " 7.906000e+00 5.985370e+01 8.391000e+00 1.051100e+00 2.068100e+00\n",
      " 5.765200e+00 1.680700e+00 1.467680e+01 1.017380e+01 3.025000e-01\n",
      " 2.175500e+00 1.448250e+01 6.017600e+00 3.974600e+00 7.552600e+00\n",
      " 4.372000e-01 1.046000e+00 5.799100e+00 5.202600e+00 1.370040e+01\n",
      " 2.844100e+00 5.354000e-01 1.306900e+00 3.059400e+00 4.995000e-01\n",
      " 1.502200e+00 1.693300e+01 1.199700e+00 2.228100e+00 1.421600e+00\n",
      " 4.152100e+00 1.910400e+00 4.225500e+00 3.850500e+00 2.251160e+01\n",
      " 1.009500e+00 2.142300e+00 7.507000e-01 1.931500e+00 3.508400e+00\n",
      " 1.446400e+00 3.781000e+00 2.401200e+00 4.110000e+00 5.763000e-01\n",
      " 3.299500e+00 7.742000e-01 1.206800e+00 3.687800e+00 4.480000e-01\n",
      " 5.231900e+00 2.347600e+00 1.049400e+00 1.030800e+00 4.367600e+00\n",
      " 1.062400e+00 2.186500e+00 7.744000e-01 6.085580e+01 3.432000e+00\n",
      " 1.808240e+01 3.215100e+00 1.840900e+00 6.749000e-01 4.623700e+00\n",
      " 2.536000e+00 1.542910e+01 1.873800e+00 2.269800e+00 3.553300e+00\n",
      " 2.161800e+00 1.789400e+00 2.849400e+00 1.651270e+01 6.624000e-01\n",
      " 1.636000e+00 1.908400e+00 4.030400e+00 1.198500e+00 6.707990e+01\n",
      " 2.997000e+00 1.680900e+01 5.156400e+00 1.653910e+01 3.927000e+00\n",
      " 3.908400e+00 8.369000e-01 2.533400e+00 9.185600e+00 2.098800e+00\n",
      " 9.135000e+00 3.905600e+00 5.352000e-01 1.157160e+01 6.076000e-01\n",
      " 1.652600e+00 1.046280e+01 3.658400e+00 1.426600e+00 4.278700e+00\n",
      " 1.312090e+01 3.729300e+00 6.944100e+00 2.904000e+00 4.011600e+00\n",
      " 7.389200e+00 9.200000e-01 2.795740e+01 3.018000e+00 2.705200e+00\n",
      " 6.546000e-01 7.948000e-01 6.997800e+00 1.048000e+00 2.008500e+00\n",
      " 6.155300e+00 3.637000e+00 3.252000e-01 2.860000e-01 1.793700e+00\n",
      " 4.780090e+01 2.353000e+00 4.034900e+00 4.444300e+00 2.965200e+00\n",
      " 9.955000e-01 3.371000e-01 2.728100e+00 4.254200e+00 1.876000e+00\n",
      " 8.727000e+00 1.728500e+00 9.506200e+00 3.332500e+00 3.546050e+01\n",
      " 2.816600e+00 2.749000e+00 1.490160e+01 6.263800e+00 2.827000e+00\n",
      " 1.634900e+00 2.122800e+00 1.387040e+01 1.271880e+01 3.239900e+00\n",
      " 6.135000e-01 2.210900e+00 3.959400e+00 1.241400e+00 4.493800e+00\n",
      " 8.834000e-01 1.656200e+00 8.989000e-01 2.887600e+00 6.067100e+00\n",
      " 2.408300e+00 3.811400e+00 1.332800e+00 9.616700e+00 1.856900e+00\n",
      " 9.378000e-01 9.625000e+00 1.141850e+01 8.823100e+00 9.321000e+00\n",
      " 3.858000e-01 2.642400e+00 2.860300e+00 3.380500e+00 2.975900e+00\n",
      " 5.839300e+00 1.518700e+00 2.035200e+00 1.859700e+00 7.953300e+00\n",
      " 6.109400e+00 4.767000e+00 4.970000e-01 6.010500e+00 4.900000e-01\n",
      " 4.562000e-01 3.680300e+00 3.043700e+00 3.880200e+00 1.104860e+01\n",
      " 3.424300e+00 4.072100e+00 1.389650e+01 9.214200e+00 3.840910e+01\n",
      " 7.511700e+00 7.210000e-01 9.510000e-02 3.441270e+01 1.642300e+00\n",
      " 3.611900e+00 2.227600e+00 2.171500e+00 2.602200e+00 5.625400e+00\n",
      " 1.723960e+01 1.164900e+00 6.905600e+00 1.197240e+01 1.616100e+00\n",
      " 5.385900e+00 1.683700e+00 1.884500e+00 5.320100e+00 1.967200e+00\n",
      " 4.923600e+00 2.000510e+01 2.925800e+00 8.769000e-01 1.759600e+00\n",
      " 1.670800e+00 2.679500e+01 1.278200e+00 3.624100e+00 3.731300e+00\n",
      " 2.932200e+00 2.040000e+00 2.631700e+00 2.041800e+00 8.632000e-01\n",
      " 1.519900e+00 5.130400e+00 6.585900e+00 5.208600e+01 3.470900e+00\n",
      " 9.190800e+00 4.619400e+00 9.610000e-01 6.121300e+00 1.880200e+00\n",
      " 1.169700e+00 7.844500e+00 4.890300e+00 7.125000e-01 8.735000e-01\n",
      " 2.413400e+00 1.126800e+00 5.465000e-01 3.001500e+00 1.579100e+00\n",
      " 5.405000e-01 2.460200e+00 1.233000e+00 1.524100e+00 1.457020e+01\n",
      " 6.407000e-01 3.252000e+00 2.792600e+00 5.103000e-01 1.175480e+01\n",
      " 1.018900e+00 2.060800e+00 3.964700e+00 8.997000e-01 9.040000e-01\n",
      " 9.279000e-01 1.574360e+01 2.244000e+00 1.425500e+00 1.897000e+00\n",
      " 1.637000e+01 8.668000e-01 1.402360e+01 3.100900e+00 9.204000e-01\n",
      " 1.338000e+00 2.947000e-01 7.737000e-01 5.771300e+00 2.415600e+00\n",
      " 2.680000e-01 1.323500e+00 5.119300e+00 3.450400e+00 1.572200e+00\n",
      " 4.008800e+00 4.659000e-01 2.081200e+00 6.668600e+00 3.440300e+00\n",
      " 1.111690e+01 1.418700e+00 2.033000e+00 4.911500e+00 3.473200e+00\n",
      " 3.982800e+00 8.322300e+00 2.459700e+00 2.913660e+01 1.302400e+00\n",
      " 2.928700e+00 1.793700e+00 8.490500e+00 4.589100e+00 1.935200e+00\n",
      " 1.257500e+01 1.734900e+00 7.182800e+00 2.330200e+00 3.716000e+00\n",
      " 1.316400e+00 3.355000e+00 8.833800e+00 1.795200e+00 7.026100e+00\n",
      " 1.036930e+01 3.630600e+00 4.284000e+00 8.785900e+00 1.835200e+00\n",
      " 1.553600e+00 5.947000e-01 1.219950e+01 3.110300e+00 5.894200e+00\n",
      " 3.897200e+00 3.433800e+00 1.692500e+00 1.289170e+01 6.104100e+00\n",
      " 2.018000e+01 7.806000e-01 2.042000e+00 3.132800e+00 1.545450e+01\n",
      " 1.128000e+00 1.863800e+00 3.900000e-01 2.652000e+00 1.744900e+00\n",
      " 1.196900e+00 1.617660e+01 9.544000e-01 8.171000e+00 3.118200e+00\n",
      " 1.541800e+00 3.933570e+01 5.275000e+00 1.010100e+00 2.058800e+00\n",
      " 7.020200e+00 2.075310e+01 4.277900e+00 1.832400e+00 2.071000e+00\n",
      " 4.741400e+00 6.275000e-01 2.670400e+00 2.555900e+00 2.824700e+00\n",
      " 2.780910e+01 1.051400e+00 1.347270e+01 1.658740e+01 2.962300e+00\n",
      " 7.824000e-01 3.974600e+00 1.938700e+00 4.186000e+00 2.521200e+01\n",
      " 1.932400e+00 1.504762e+02 2.476800e+00 1.056210e+01 2.613000e+00\n",
      " 7.108800e+00 2.040100e+00 3.458700e+00 3.252400e+00 3.693000e-01\n",
      " 3.008800e+00 1.422200e+00 5.112400e+00 5.093200e+00 4.573600e+00\n",
      " 3.965300e+00 5.834000e-01 8.920500e+00 1.377600e+00 5.457000e-01\n",
      " 5.421900e+00 3.989000e-01 3.052800e+00 5.342100e+00 9.276000e-01\n",
      " 1.517320e+01 7.141000e-01 3.246000e-01 7.882800e+00 2.782200e+00\n",
      " 2.575800e+00 8.851900e+00 2.532200e+00 8.385500e+00 4.986100e+00\n",
      " 2.464500e+00 4.087500e+00 7.901000e-01 2.705400e+00 2.220100e+01\n",
      " 1.136610e+01 1.341000e+00 3.011000e-01 2.708800e+00 1.843600e+00\n",
      " 1.064120e+01 7.000000e-01 1.224700e+00 8.867800e+00 2.095213e+02\n",
      " 5.993700e+00 4.406600e+00 7.023400e+00 6.979200e+00 1.093600e+00\n",
      " 1.122200e+00 3.276600e+00 1.066340e+01 4.899400e+00 7.189300e+00\n",
      " 1.988000e-01 1.526800e+00 1.697360e+01 8.594200e+00 2.654000e+00\n",
      " 2.173700e+00 6.686170e+01 3.437400e+00 5.517100e+00 3.023700e+00\n",
      " 5.697200e+00 9.280400e+00 4.510400e+00 5.441000e+00 2.116000e+00\n",
      " 3.199900e+00 6.705800e+00 7.324400e+00 6.662500e+00 1.936000e+00\n",
      " 2.869260e+01 2.352700e+00 1.000000e-02 4.564300e+00 1.412800e+00\n",
      " 5.296800e+00 3.386900e+00 1.336700e+00 1.774650e+01 2.835000e+00\n",
      " 5.110000e-01 1.592300e+00 4.288600e+00 8.488000e-01 4.227100e+00\n",
      " 8.029400e+00 6.313200e+00 3.223400e+00 5.152000e-01 3.109000e-01\n",
      " 1.008450e+01 2.775800e+00 2.497100e+00 7.942400e+00 4.512450e+01\n",
      " 1.197120e+01 1.455600e+00 3.450100e+00 8.993400e+00 2.149700e+00\n",
      " 7.512000e+00 1.059370e+01 1.200700e+00 2.802200e+00 2.506600e+01\n",
      " 9.206100e+00 3.007400e+00 7.766400e+00 2.836000e-01 1.535200e+00\n",
      " 1.025450e+01 4.502200e+00 1.030770e+01 3.193000e+00 8.574000e-01\n",
      " 8.415000e-01 3.456600e+00 4.135000e-01 1.027600e+00 1.878380e+01\n",
      " 9.571000e-01 2.735200e+00 2.127200e+00 3.872300e+00 1.541000e+00\n",
      " 3.711400e+00 6.632500e+00 2.668310e+01 1.098400e+00 2.856000e+00\n",
      " 9.488000e-01 2.485100e+00 3.246000e+00 2.269800e+00 2.707500e+00\n",
      " 2.344800e+00 4.450000e+00 6.819000e-01 4.545400e+00 7.623000e-01\n",
      " 2.014200e+00 4.364200e+00 5.423000e-01 5.734800e+00 2.946600e+00\n",
      " 1.157100e+00 9.292000e-01 5.222400e+00 1.393500e+00 2.431800e+00\n",
      " 6.426000e-01 2.156202e+02 2.898800e+00 1.731300e+01 2.493300e+00\n",
      " 2.922100e+00 2.143400e+00 4.868800e+00 2.674000e+00 1.270070e+01\n",
      " 2.914300e+00 1.414100e+00 2.981200e+00 2.363100e+00 1.064000e+00\n",
      " 2.672000e+00 2.219590e+01 7.788000e-01 2.520900e+00 2.206600e+00\n",
      " 3.272900e+00 1.535400e+00 6.455060e+01 2.315000e+00 2.078820e+01\n",
      " 7.439200e+00 1.806090e+01 6.031900e+00 6.003300e+00 9.428000e-01\n",
      " 1.288600e+00 1.829710e+01 1.719300e+00 1.165940e+01 6.883000e+00\n",
      " 5.945000e-01 1.898970e+01 8.226000e-01 1.646900e+00 2.122660e+01\n",
      " 2.300400e+00 1.173300e+00 4.180600e+00 1.559430e+01 3.195800e+00\n",
      " 1.242400e+01 2.805400e+00 2.211800e+00 6.479200e+00 3.039400e+00\n",
      " 2.883050e+01 8.338900e+00 1.854400e+00 8.921000e-01 5.095000e-01\n",
      " 6.070600e+00 3.828900e+00 1.860700e+00 6.841400e+00 7.804000e+00\n",
      " 2.236000e-01 3.489000e-01 3.039700e+00 5.235770e+01 3.142100e+00\n",
      " 5.249500e+00 6.134700e+00 2.522700e+00 1.120200e+00 4.101000e-01\n",
      " 2.247700e+00 2.711400e+00 2.186900e+00 4.925700e+00 1.510300e+00\n",
      " 1.067310e+01 9.555900e+00 4.120830e+01 5.012300e+00 1.472000e+00\n",
      " 1.666080e+01 5.996200e+00 2.136200e+00 2.465900e+00 1.689300e+00\n",
      " 2.317240e+01 1.990400e+01 2.539600e+00 1.129300e+00 1.975600e+00\n",
      " 9.097500e+00 2.242100e+00 6.205100e+00 8.646000e-01 7.092000e-01\n",
      " 1.129600e+00 3.449300e+00 7.517100e+00 2.356500e+00 4.839200e+00\n",
      " 3.684600e+00 1.442840e+01 1.463300e+00 7.496000e-01 1.335510e+01\n",
      " 1.153830e+01 9.931300e+00 1.290600e+01 3.699000e-01 2.701000e+00\n",
      " 2.627800e+00 3.258700e+00 5.206100e+00 7.099000e+00 7.795500e+00\n",
      " 2.133200e+00 1.982400e+00 3.715280e+01 6.568600e+00 5.604300e+00]\n"
     ]
    }
   ],
   "source": [
    "X_arr = X.to_numpy().astype(np.float32)\n",
    "Y_arr = Y.to_numpy().astype(np.float32)\n",
    "\n",
    "print(type(X_arr))\n",
    "print(type(Y_arr))\n",
    "\n",
    "\n",
    "print(\"type:\",type(X_arr),X_arr)\n",
    "print(\"data_type:\",Y_arr.dtype, Y_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TWpN0nVTpUx"
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ1FKEs-4btX"
   },
   "source": [
    "### Normalize data\n",
    "- Normalize features\n",
    "- Use tf.math.l2_normalize to normalize features\n",
    "- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0Tfe00X78wB"
   },
   "outputs": [],
   "source": [
    "X = tf.math.l2_normalize(\n",
    "    X_arr,\n",
    "    axis=None,\n",
    "    epsilon=1e-12,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmXUGc2oTspa"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJelDMpzxs0L"
   },
   "source": [
    "### Define weight and bias\n",
    "- Initialize weight and bias with tf.zeros\n",
    "- tf.zeros is an initializer that generates tensors initialized to 0\n",
    "- Specify the value for shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11842,
     "status": "ok",
     "timestamp": 1580042225113,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "8o9RPWVTxs0O",
    "outputId": "d5df0d38-4cfe-410e-acd7-6a8293ba1a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]], shape=(4, 1), dtype=float32)\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = tf.zeros(shape=(4, 1)) \n",
    "b = tf.zeros(shape=(1))\n",
    "\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a0wr94aTyjg"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMXXYdOSxs0Q"
   },
   "source": [
    "### Get prediction\n",
    "- Define a function to get prediction\n",
    "- Approach: prediction = (X * W) + b; here is X is features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8Cty1y0xs0S"
   },
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def prediction(features, W, b):     \n",
    "  y_pred = tf.add(tf.matmul(features, W), b)     \n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQmS3Tauxs0V"
   },
   "source": [
    "### Calculate loss\n",
    "- Calculate loss using predictions\n",
    "- Define a function to calculate loss\n",
    "- We are calculating mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FRXmDd5xs0X"
   },
   "outputs": [],
   "source": [
    " @tf.function\n",
    " def loss(y_actual, y_predicted):\n",
    "   diff = y_actual - y_predicted   \n",
    "   sqr = tf.square(diff)     \n",
    "   avg = tf.reduce_mean(sqr)     \n",
    "   return avg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbBpnOtfT0wd"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkOzAUUsTmF_"
   },
   "source": [
    "### Define a function to train the model\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R4uieGYLYtM"
   },
   "outputs": [],
   "source": [
    "def train(X, y_actual, w, b, learning_rate=0.01):\n",
    "  # Record mathematical operations on 'tape' to calculate loss     \n",
    "  with tf.GradientTape() as t:         \n",
    "    t.watch([w,b])         \n",
    "    current_prediction = prediction(X, w, b)         \n",
    "    current_loss = loss(y_actual, current_prediction)          \n",
    "  # Calculate Gradients for Loss with respect to Weights and Bias     \n",
    "  dw, db = t.gradient(current_loss,[w, b])          \n",
    "  # Update Weights and Bias     \n",
    "  w = w - learning_rate * dw     \n",
    "  b = b - learning_rate * db          \n",
    "  return w, b \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW4SEP8kT2ls"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeN0deOvT81N"
   },
   "source": [
    "### Train the model for 100 epochs \n",
    "- Observe the training loss at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12739,
     "status": "ok",
     "timestamp": 1580042226117,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "Jjkn4gUgLevE",
    "outputId": "bdda88e4-6920-47d1-a818-e6168a213a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.02202894 0.02245906 0.02182905 0.02253223]\n",
      " [0.02235197 0.02141321 0.02140607 0.02240551]\n",
      " [0.0207707  0.02051548 0.02051192 0.02137037]\n",
      " ...\n",
      " [0.00505436 0.00513467 0.00499903 0.00514181]\n",
      " [0.00785282 0.0079956  0.0078082  0.00799738]\n",
      " [0.00643931 0.00662849 0.00642682 0.00664455]], shape=(1000, 4), dtype=float32)\n",
      "Current Training Loss on iteration 0 236.16759\n",
      "Current Training Loss on iteration 1 235.09299\n",
      "Current Training Loss on iteration 2 234.06102\n",
      "Current Training Loss on iteration 3 233.06998\n",
      "Current Training Loss on iteration 4 232.11781\n",
      "Current Training Loss on iteration 5 231.20381\n",
      "Current Training Loss on iteration 6 230.32585\n",
      "Current Training Loss on iteration 7 229.48265\n",
      "Current Training Loss on iteration 8 228.67297\n",
      "Current Training Loss on iteration 9 227.89539\n",
      "Current Training Loss on iteration 10 227.14832\n",
      "Current Training Loss on iteration 11 226.43114\n",
      "Current Training Loss on iteration 12 225.74234\n",
      "Current Training Loss on iteration 13 225.0808\n",
      "Current Training Loss on iteration 14 224.44553\n",
      "Current Training Loss on iteration 15 223.835\n",
      "Current Training Loss on iteration 16 223.24931\n",
      "Current Training Loss on iteration 17 222.68655\n",
      "Current Training Loss on iteration 18 222.14595\n",
      "Current Training Loss on iteration 19 221.62688\n",
      "Current Training Loss on iteration 20 221.12842\n",
      "Current Training Loss on iteration 21 220.6498\n",
      "Current Training Loss on iteration 22 220.18993\n",
      "Current Training Loss on iteration 23 219.74841\n",
      "Current Training Loss on iteration 24 219.32425\n",
      "Current Training Loss on iteration 25 218.91705\n",
      "Current Training Loss on iteration 26 218.52579\n",
      "Current Training Loss on iteration 27 218.15027\n",
      "Current Training Loss on iteration 28 217.78957\n",
      "Current Training Loss on iteration 29 217.44315\n",
      "Current Training Loss on iteration 30 217.11034\n",
      "Current Training Loss on iteration 31 216.79077\n",
      "Current Training Loss on iteration 32 216.48412\n",
      "Current Training Loss on iteration 33 216.18932\n",
      "Current Training Loss on iteration 34 215.90611\n",
      "Current Training Loss on iteration 35 215.63437\n",
      "Current Training Loss on iteration 36 215.37334\n",
      "Current Training Loss on iteration 37 215.12263\n",
      "Current Training Loss on iteration 38 214.88185\n",
      "Current Training Loss on iteration 39 214.65053\n",
      "Current Training Loss on iteration 40 214.42859\n",
      "Current Training Loss on iteration 41 214.21527\n",
      "Current Training Loss on iteration 42 214.01044\n",
      "Current Training Loss on iteration 43 213.81369\n",
      "Current Training Loss on iteration 44 213.62482\n",
      "Current Training Loss on iteration 45 213.44333\n",
      "Current Training Loss on iteration 46 213.2691\n",
      "Current Training Loss on iteration 47 213.10179\n",
      "Current Training Loss on iteration 48 212.94106\n",
      "Current Training Loss on iteration 49 212.78687\n",
      "Current Training Loss on iteration 50 212.63857\n",
      "Current Training Loss on iteration 51 212.4961\n",
      "Current Training Loss on iteration 52 212.35948\n",
      "Current Training Loss on iteration 53 212.22816\n",
      "Current Training Loss on iteration 54 212.10205\n",
      "Current Training Loss on iteration 55 211.98093\n",
      "Current Training Loss on iteration 56 211.86461\n",
      "Current Training Loss on iteration 57 211.75293\n",
      "Current Training Loss on iteration 58 211.64558\n",
      "Current Training Loss on iteration 59 211.54266\n",
      "Current Training Loss on iteration 60 211.44374\n",
      "Current Training Loss on iteration 61 211.34874\n",
      "Current Training Loss on iteration 62 211.2575\n",
      "Current Training Loss on iteration 63 211.16982\n",
      "Current Training Loss on iteration 64 211.08572\n",
      "Current Training Loss on iteration 65 211.00484\n",
      "Current Training Loss on iteration 66 210.92729\n",
      "Current Training Loss on iteration 67 210.85278\n",
      "Current Training Loss on iteration 68 210.78104\n",
      "Current Training Loss on iteration 69 210.71225\n",
      "Current Training Loss on iteration 70 210.64633\n",
      "Current Training Loss on iteration 71 210.58289\n",
      "Current Training Loss on iteration 72 210.52199\n",
      "Current Training Loss on iteration 73 210.46347\n",
      "Current Training Loss on iteration 74 210.4073\n",
      "Current Training Loss on iteration 75 210.3533\n",
      "Current Training Loss on iteration 76 210.30153\n",
      "Current Training Loss on iteration 77 210.2518\n",
      "Current Training Loss on iteration 78 210.204\n",
      "Current Training Loss on iteration 79 210.15811\n",
      "Current Training Loss on iteration 80 210.1141\n",
      "Current Training Loss on iteration 81 210.0717\n",
      "Current Training Loss on iteration 82 210.03108\n",
      "Current Training Loss on iteration 83 209.99205\n",
      "Current Training Loss on iteration 84 209.95453\n",
      "Current Training Loss on iteration 85 209.91852\n",
      "Current Training Loss on iteration 86 209.88396\n",
      "Current Training Loss on iteration 87 209.85075\n",
      "Current Training Loss on iteration 88 209.81891\n",
      "Current Training Loss on iteration 89 209.78818\n",
      "Current Training Loss on iteration 90 209.75874\n",
      "Current Training Loss on iteration 91 209.73053\n",
      "Current Training Loss on iteration 92 209.70349\n",
      "Current Training Loss on iteration 93 209.67744\n",
      "Current Training Loss on iteration 94 209.65225\n",
      "Current Training Loss on iteration 95 209.62833\n",
      "Current Training Loss on iteration 96 209.6052\n",
      "Current Training Loss on iteration 97 209.58302\n",
      "Current Training Loss on iteration 98 209.56174\n",
      "Current Training Loss on iteration 99 209.5413\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "for i in range(100):         \n",
    "    #W, b = train(X, Y_arr, W, b)   \n",
    "    W, b = train(X,Y_arr, W, b)  \n",
    "    print('Current Training Loss on iteration', i, loss(Y_arr, prediction(X, W, b)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vanvD93FV0_k"
   },
   "source": [
    "### Observe values of Weight\n",
    "- Print the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12731,
     "status": "ok",
     "timestamp": 1580042226118,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "QSqpy4gtWaOD",
    "outputId": "0f9cd889-8660-48cf-9613-bdc09220192a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.0547828 ]\n",
      " [0.05488191]\n",
      " [0.05422735]\n",
      " [0.05533117]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9KpRupYUEwy"
   },
   "source": [
    "### Observe values of Bias\n",
    "- Print the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12724,
     "status": "ok",
     "timestamp": 1580042226119,
     "user": {
      "displayName": "Rajendra Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA40YBZ3VXmXiZh01WwNVubjn3DrP_DRkO0UgTIag=s64",
      "userId": "03358074869551673090"
     },
     "user_tz": -330
    },
    "id": "bhEWkGqHWohg",
    "outputId": "dd6138cc-e227-4d84-e69e-afad926ea15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.607657], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Questions - Internal - R6 AIML Labs .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
