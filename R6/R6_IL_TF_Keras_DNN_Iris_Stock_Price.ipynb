{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUZjPnVXGz0Z"
   },
   "source": [
    "# The Iris Dataset\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMbmpriavLE9"
   },
   "source": [
    "### Specifying the TensorFlow version\n",
    "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu8bUU__oa7h"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLz1Ckvfvn6D"
   },
   "source": [
    "### Import TensorFlow\n",
    "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWrzVTLOvn6M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uYeJgkNuXNC"
   },
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcASNsewsfQX"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-vVQBBqg7DI"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kE0EDKvQhEIe"
   },
   "source": [
    "### Import dataset\n",
    "- Import iris dataset\n",
    "- Import the dataset using sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOOWpD26Haq3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': '/Users/ahmedshadab/Documents/machine_learning/anaconda3/lib/python3.7/site-packages/sklearn/datasets/data/iris.csv'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"pricesandiris/iris.csv\")\n",
    "#Importing using sklearn library\n",
    "from sklearn import datasets\n",
    "iris_ds = datasets.load_iris()\n",
    "iris_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ta8YqInTh5v5"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HERt3drbhX0i"
   },
   "source": [
    "### Get features and label from the dataset in separate variable\n",
    "- you can get the features using .data method\n",
    "- you can get the features using .target method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cV-_qHAHyvE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "X = iris_ds.data\n",
    "Y = iris_ds.target\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qg1A2lkUjFak"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### Create train and test data\n",
    "- use train_test_split to get train and test set\n",
    "- set a random_state\n",
    "- test_size: 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYKNJL85h7pQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0KVP17Ozaix"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIjqxbhWv1zv"
   },
   "source": [
    "### One-hot encode the labels\n",
    "- convert class vectors (integers) to binary class matrix\n",
    "- convert labels\n",
    "- number of classes: 3\n",
    "- we are doing this to use categorical_crossentropy as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9vv-_gpyLY9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "#Encoding the output class label (One-Hot Encoding)\n",
    "y_train=to_categorical(y_train,3,dtype='int')\n",
    "y_test=to_categorical(y_test,3,dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovjLyYzWkO9s"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbIFzoPNSyYo"
   },
   "source": [
    "### Initialize a sequential model\n",
    "- Define a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FvSbf1UjHtl"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "#Initialize Sequential Graph (model)\n",
    "model = tf.keras.Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGMy999vlacX"
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72ibK5Jxm8iL"
   },
   "source": [
    "### Add a layer\n",
    "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
    "- Apply Softmax on Dense Layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZKrBNSRm_o9"
   },
   "outputs": [],
   "source": [
    "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
    "model.add(Dense(10, activation='relu', input_shape=(4,)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4uiTH8plmNX"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJL8n8vcSyYz"
   },
   "source": [
    "### Compile the model\n",
    "- Use SGD as Optimizer\n",
    "- Use categorical_crossentropy as loss function\n",
    "- Use accuracy as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tc_-fjIEk1ve"
   },
   "outputs": [],
   "source": [
    "#Compile the model - add Loss and Gradient Descent optimizer\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sihIGbRll_jT"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54ZZCfNGlu0i"
   },
   "source": [
    "### Summarize the model\n",
    "- Check model layers\n",
    "- Understand number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elER3F_4ln8n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PiP7j3Vmj4p"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWdbfFCXmCHt"
   },
   "source": [
    "### Fit the model\n",
    "- Give train data as training features and labels\n",
    "- Epochs: 100\n",
    "- Give validation data as testing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cO1c-5tjmBVZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 1s 9ms/sample - loss: 1.3677 - accuracy: 0.4762 - val_loss: 1.2196 - val_accuracy: 0.3111\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 490us/sample - loss: 1.1658 - accuracy: 0.3429 - val_loss: 1.1172 - val_accuracy: 0.3111\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 475us/sample - loss: 1.0765 - accuracy: 0.3429 - val_loss: 1.0544 - val_accuracy: 0.3111\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 462us/sample - loss: 1.0328 - accuracy: 0.3429 - val_loss: 1.0245 - val_accuracy: 0.3111\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 508us/sample - loss: 0.9982 - accuracy: 0.3429 - val_loss: 0.9955 - val_accuracy: 0.3111\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 693us/sample - loss: 0.9685 - accuracy: 0.3429 - val_loss: 0.9612 - val_accuracy: 0.3111\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 517us/sample - loss: 0.9425 - accuracy: 0.3429 - val_loss: 0.9379 - val_accuracy: 0.5333\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 475us/sample - loss: 0.9265 - accuracy: 0.3905 - val_loss: 0.9211 - val_accuracy: 0.3556\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 443us/sample - loss: 0.9061 - accuracy: 0.3714 - val_loss: 0.9057 - val_accuracy: 0.3111\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 472us/sample - loss: 0.8862 - accuracy: 0.3524 - val_loss: 0.8869 - val_accuracy: 0.4889\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 491us/sample - loss: 0.8702 - accuracy: 0.4381 - val_loss: 0.8733 - val_accuracy: 0.4444\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 481us/sample - loss: 0.8588 - accuracy: 0.4095 - val_loss: 0.8596 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 458us/sample - loss: 0.8463 - accuracy: 0.5524 - val_loss: 0.8447 - val_accuracy: 0.5333\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 481us/sample - loss: 0.8301 - accuracy: 0.4667 - val_loss: 0.8329 - val_accuracy: 0.4000\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 477us/sample - loss: 0.8120 - accuracy: 0.4286 - val_loss: 0.8220 - val_accuracy: 0.4222\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 510us/sample - loss: 0.7998 - accuracy: 0.4286 - val_loss: 0.8206 - val_accuracy: 0.3111\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 472us/sample - loss: 0.7884 - accuracy: 0.3619 - val_loss: 0.8028 - val_accuracy: 0.3111\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 482us/sample - loss: 0.7726 - accuracy: 0.3714 - val_loss: 0.7786 - val_accuracy: 0.6000\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 464us/sample - loss: 0.7578 - accuracy: 0.5714 - val_loss: 0.7842 - val_accuracy: 0.5556\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 459us/sample - loss: 0.7474 - accuracy: 0.5619 - val_loss: 0.7576 - val_accuracy: 0.5556\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 465us/sample - loss: 0.7317 - accuracy: 0.6667 - val_loss: 0.7412 - val_accuracy: 0.8222\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 471us/sample - loss: 0.7244 - accuracy: 0.8000 - val_loss: 0.7322 - val_accuracy: 0.7111\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 459us/sample - loss: 0.7107 - accuracy: 0.7905 - val_loss: 0.7293 - val_accuracy: 0.6222\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 466us/sample - loss: 0.7014 - accuracy: 0.7429 - val_loss: 0.7142 - val_accuracy: 0.7556\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 465us/sample - loss: 0.6915 - accuracy: 0.7810 - val_loss: 0.7027 - val_accuracy: 0.8222\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 471us/sample - loss: 0.6830 - accuracy: 0.8571 - val_loss: 0.7042 - val_accuracy: 0.6222\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.6793 - accuracy: 0.7810 - val_loss: 0.6857 - val_accuracy: 0.8222\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 475us/sample - loss: 0.6681 - accuracy: 0.8476 - val_loss: 0.6753 - val_accuracy: 0.9333\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 458us/sample - loss: 0.6589 - accuracy: 0.9048 - val_loss: 0.6698 - val_accuracy: 0.8444\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 477us/sample - loss: 0.6506 - accuracy: 0.8476 - val_loss: 0.6603 - val_accuracy: 0.8667\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 474us/sample - loss: 0.6422 - accuracy: 0.8762 - val_loss: 0.6531 - val_accuracy: 0.9556\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 449us/sample - loss: 0.6358 - accuracy: 0.9238 - val_loss: 0.6433 - val_accuracy: 0.9556\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 624us/sample - loss: 0.6262 - accuracy: 0.9333 - val_loss: 0.6400 - val_accuracy: 0.8444\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 491us/sample - loss: 0.6215 - accuracy: 0.8381 - val_loss: 0.6359 - val_accuracy: 0.8000\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 471us/sample - loss: 0.6119 - accuracy: 0.8381 - val_loss: 0.6267 - val_accuracy: 0.8000\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 486us/sample - loss: 0.5999 - accuracy: 0.8286 - val_loss: 0.6064 - val_accuracy: 0.9778\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 466us/sample - loss: 0.5922 - accuracy: 0.8952 - val_loss: 0.5910 - val_accuracy: 0.9778\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 470us/sample - loss: 0.5736 - accuracy: 0.9333 - val_loss: 0.5674 - val_accuracy: 0.9778\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 460us/sample - loss: 0.5565 - accuracy: 0.9524 - val_loss: 0.5512 - val_accuracy: 0.9556\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 493us/sample - loss: 0.5395 - accuracy: 0.9619 - val_loss: 0.5837 - val_accuracy: 0.7111\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 450us/sample - loss: 0.5404 - accuracy: 0.8571 - val_loss: 0.5398 - val_accuracy: 0.9111\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 466us/sample - loss: 0.5191 - accuracy: 0.9524 - val_loss: 0.5193 - val_accuracy: 0.9556\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 467us/sample - loss: 0.5021 - accuracy: 0.9238 - val_loss: 0.5132 - val_accuracy: 0.9778\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 469us/sample - loss: 0.4984 - accuracy: 0.9238 - val_loss: 0.4972 - val_accuracy: 0.9778\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 450us/sample - loss: 0.4923 - accuracy: 0.9238 - val_loss: 0.4937 - val_accuracy: 0.9556\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 442us/sample - loss: 0.4802 - accuracy: 0.9238 - val_loss: 0.4857 - val_accuracy: 0.9778\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 446us/sample - loss: 0.4730 - accuracy: 0.9429 - val_loss: 0.4808 - val_accuracy: 0.9556\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 444us/sample - loss: 0.4721 - accuracy: 0.9238 - val_loss: 0.4734 - val_accuracy: 0.9556\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.4652 - accuracy: 0.9143 - val_loss: 0.4573 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 458us/sample - loss: 0.4568 - accuracy: 0.9238 - val_loss: 0.4672 - val_accuracy: 0.8889\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 458us/sample - loss: 0.4434 - accuracy: 0.9619 - val_loss: 0.4410 - val_accuracy: 0.9778\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 447us/sample - loss: 0.4275 - accuracy: 0.9429 - val_loss: 0.4354 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 453us/sample - loss: 0.4227 - accuracy: 0.9714 - val_loss: 0.4433 - val_accuracy: 0.8889\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 448us/sample - loss: 0.4221 - accuracy: 0.9238 - val_loss: 0.4283 - val_accuracy: 0.9556\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 447us/sample - loss: 0.4104 - accuracy: 0.9429 - val_loss: 0.4132 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 437us/sample - loss: 0.4012 - accuracy: 0.9714 - val_loss: 0.4091 - val_accuracy: 0.9778\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 438us/sample - loss: 0.3967 - accuracy: 0.9524 - val_loss: 0.4038 - val_accuracy: 0.9556\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 452us/sample - loss: 0.3866 - accuracy: 0.9333 - val_loss: 0.3934 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 454us/sample - loss: 0.3830 - accuracy: 0.9714 - val_loss: 0.3991 - val_accuracy: 0.9111\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 454us/sample - loss: 0.3778 - accuracy: 0.9333 - val_loss: 0.3840 - val_accuracy: 0.9556\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.3700 - accuracy: 0.9429 - val_loss: 0.4013 - val_accuracy: 0.8889\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 448us/sample - loss: 0.3728 - accuracy: 0.9429 - val_loss: 0.3670 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 457us/sample - loss: 0.3555 - accuracy: 0.9619 - val_loss: 0.3753 - val_accuracy: 0.9333\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 448us/sample - loss: 0.3599 - accuracy: 0.9333 - val_loss: 0.3544 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 449us/sample - loss: 0.3454 - accuracy: 0.9714 - val_loss: 0.3487 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 454us/sample - loss: 0.3425 - accuracy: 0.9619 - val_loss: 0.3458 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 451us/sample - loss: 0.3398 - accuracy: 0.9619 - val_loss: 0.3423 - val_accuracy: 0.9778\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 465us/sample - loss: 0.3364 - accuracy: 0.9524 - val_loss: 0.3475 - val_accuracy: 0.9556\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 455us/sample - loss: 0.3277 - accuracy: 0.9524 - val_loss: 0.3310 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 461us/sample - loss: 0.3226 - accuracy: 0.9714 - val_loss: 0.3240 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 459us/sample - loss: 0.3171 - accuracy: 0.9714 - val_loss: 0.3238 - val_accuracy: 0.9778\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 439us/sample - loss: 0.3180 - accuracy: 0.9143 - val_loss: 0.3398 - val_accuracy: 0.9333\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 460us/sample - loss: 0.3202 - accuracy: 0.9524 - val_loss: 0.3662 - val_accuracy: 0.8222\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 455us/sample - loss: 0.3219 - accuracy: 0.9429 - val_loss: 0.3181 - val_accuracy: 0.9778\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 457us/sample - loss: 0.3095 - accuracy: 0.9333 - val_loss: 0.3114 - val_accuracy: 0.9556\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 473us/sample - loss: 0.2991 - accuracy: 0.9524 - val_loss: 0.3596 - val_accuracy: 0.8222\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 474us/sample - loss: 0.3240 - accuracy: 0.8952 - val_loss: 0.3031 - val_accuracy: 0.9556\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 467us/sample - loss: 0.2928 - accuracy: 0.9429 - val_loss: 0.3168 - val_accuracy: 0.9333\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 486us/sample - loss: 0.2995 - accuracy: 0.9333 - val_loss: 0.2950 - val_accuracy: 0.9778\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.3030 - accuracy: 0.9429 - val_loss: 0.2851 - val_accuracy: 0.9778\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 458us/sample - loss: 0.2814 - accuracy: 0.9714 - val_loss: 0.2823 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 445us/sample - loss: 0.2788 - accuracy: 0.9714 - val_loss: 0.2933 - val_accuracy: 0.9556\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 451us/sample - loss: 0.2779 - accuracy: 0.9619 - val_loss: 0.2939 - val_accuracy: 0.9556\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 460us/sample - loss: 0.2807 - accuracy: 0.9714 - val_loss: 0.3023 - val_accuracy: 0.9111\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 453us/sample - loss: 0.2824 - accuracy: 0.9429 - val_loss: 0.2861 - val_accuracy: 0.9556\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 471us/sample - loss: 0.2732 - accuracy: 0.9524 - val_loss: 0.2661 - val_accuracy: 0.9778\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 481us/sample - loss: 0.2685 - accuracy: 0.9619 - val_loss: 0.2641 - val_accuracy: 0.9778\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 472us/sample - loss: 0.2613 - accuracy: 0.9714 - val_loss: 0.2603 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 491us/sample - loss: 0.2596 - accuracy: 0.9619 - val_loss: 0.2721 - val_accuracy: 0.9556\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 471us/sample - loss: 0.2663 - accuracy: 0.9524 - val_loss: 0.2618 - val_accuracy: 0.9778\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 487us/sample - loss: 0.2577 - accuracy: 0.9524 - val_loss: 0.2563 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 476us/sample - loss: 0.2712 - accuracy: 0.9333 - val_loss: 0.2572 - val_accuracy: 0.9778\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 444us/sample - loss: 0.2534 - accuracy: 0.9619 - val_loss: 0.2465 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 443us/sample - loss: 0.2495 - accuracy: 0.9619 - val_loss: 0.2465 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 455us/sample - loss: 0.2583 - accuracy: 0.9619 - val_loss: 0.2448 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 454us/sample - loss: 0.2441 - accuracy: 0.9524 - val_loss: 0.2441 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 452us/sample - loss: 0.2491 - accuracy: 0.9714 - val_loss: 0.2372 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 458us/sample - loss: 0.2435 - accuracy: 0.9619 - val_loss: 0.2358 - val_accuracy: 0.9778\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 473us/sample - loss: 0.2496 - accuracy: 0.9429 - val_loss: 0.2598 - val_accuracy: 0.9333\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 458us/sample - loss: 0.2405 - accuracy: 0.9429 - val_loss: 0.2384 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a424cde10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "re9ItAR3yS3J"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liw0IFf9yVqH"
   },
   "source": [
    "### Make predictions\n",
    "- Predict labels on one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5sBybi6mlLl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9489073 , 0.0497096 , 0.00138312]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSUgMq3m0bG7"
   },
   "source": [
    "### Compare the prediction with actual label\n",
    "- Print the same row as done in the previous step but of actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5WbwVPyz-qQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 104us/sample - loss: 0.2454 - accuracy: 0.9778\n",
      "[0.23839293585883248, 0.9777778]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrTKwbgE7NFT"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1UBYPNp5Tn1"
   },
   "source": [
    "# Stock prices dataset\n",
    "The data is of stock exchange's stock listings for each trading day of 2010 to 2016.\n",
    "\n",
    "## Description\n",
    "A brief description of columns.\n",
    "- open: The opening market price of the equity symbol on the date\n",
    "- high: The highest market price of the equity symbol on the date\n",
    "- low: The lowest recorded market price of the equity symbol on the date\n",
    "- close: The closing recorded price of the equity symbol on the date\n",
    "- symbol: Symbol of the listed company\n",
    "- volume: Total traded volume of the equity symbol on the date\n",
    "- date: Date of record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctH_ZW5g-M3g"
   },
   "source": [
    "### Specifying the TensorFlow version\n",
    "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQbdODpH-M3r"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFQWH1tj-M38"
   },
   "source": [
    "### Import TensorFlow\n",
    "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ho5n-xhd-M3_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgkl0qu6-M4F"
   },
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKgTyuA3-M4G"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_88voqAH-O6J"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRHCeJqP-evf"
   },
   "source": [
    "### Load the data\n",
    "- load the csv file and read it using pandas\n",
    "- file name is prices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKVH5v7r-RmC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-01-12 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.510002</td>\n",
       "      <td>115.550003</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>116.059998</td>\n",
       "      <td>1098000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-01-13 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.459999</td>\n",
       "      <td>112.849998</td>\n",
       "      <td>112.589996</td>\n",
       "      <td>117.070000</td>\n",
       "      <td>949600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-01-14 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.510002</td>\n",
       "      <td>114.379997</td>\n",
       "      <td>110.050003</td>\n",
       "      <td>115.029999</td>\n",
       "      <td>785300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-01-15 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.330002</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>111.919998</td>\n",
       "      <td>114.879997</td>\n",
       "      <td>1093700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-01-19 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.660004</td>\n",
       "      <td>110.379997</td>\n",
       "      <td>109.870003</td>\n",
       "      <td>115.870003</td>\n",
       "      <td>1523500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-01-20 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>109.059998</td>\n",
       "      <td>109.300003</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>111.599998</td>\n",
       "      <td>1653900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-01-21 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>109.730003</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>110.580002</td>\n",
       "      <td>944300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-01-22 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>111.879997</td>\n",
       "      <td>111.949997</td>\n",
       "      <td>110.190002</td>\n",
       "      <td>112.949997</td>\n",
       "      <td>744900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>111.320000</td>\n",
       "      <td>110.120003</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>114.629997</td>\n",
       "      <td>703800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>110.419998</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>107.300003</td>\n",
       "      <td>111.400002</td>\n",
       "      <td>563100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-01-27 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>110.769997</td>\n",
       "      <td>110.709999</td>\n",
       "      <td>109.019997</td>\n",
       "      <td>112.570000</td>\n",
       "      <td>896100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-01-28 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>110.900002</td>\n",
       "      <td>112.580002</td>\n",
       "      <td>109.900002</td>\n",
       "      <td>112.970001</td>\n",
       "      <td>680400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-01-29 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.349998</td>\n",
       "      <td>114.470001</td>\n",
       "      <td>111.669998</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>749900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-02-01 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>112.900002</td>\n",
       "      <td>114.849998</td>\n",
       "      <td>574200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-02-02 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.250000</td>\n",
       "      <td>110.559998</td>\n",
       "      <td>109.750000</td>\n",
       "      <td>113.860001</td>\n",
       "      <td>694800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-02-03 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.379997</td>\n",
       "      <td>114.050003</td>\n",
       "      <td>109.639999</td>\n",
       "      <td>114.639999</td>\n",
       "      <td>896300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-02-04 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>115.709999</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>116.320000</td>\n",
       "      <td>956300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-02-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.120003</td>\n",
       "      <td>114.019997</td>\n",
       "      <td>109.709999</td>\n",
       "      <td>116.489998</td>\n",
       "      <td>997100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-02-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>113.300003</td>\n",
       "      <td>111.160004</td>\n",
       "      <td>110.459999</td>\n",
       "      <td>113.300003</td>\n",
       "      <td>1200500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-02-09 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>111.169998</td>\n",
       "      <td>110.650002</td>\n",
       "      <td>109.639999</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>1725200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-02-10 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>106.730003</td>\n",
       "      <td>107.519997</td>\n",
       "      <td>106.360001</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>1946000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016-02-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>105.629997</td>\n",
       "      <td>107.129997</td>\n",
       "      <td>104.110001</td>\n",
       "      <td>109.260002</td>\n",
       "      <td>1319500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2016-02-12 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>108.559998</td>\n",
       "      <td>107.839996</td>\n",
       "      <td>107.070000</td>\n",
       "      <td>109.430000</td>\n",
       "      <td>922400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016-02-16 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>109.110001</td>\n",
       "      <td>110.769997</td>\n",
       "      <td>107.010002</td>\n",
       "      <td>111.300003</td>\n",
       "      <td>1185100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016-02-17 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>110.830002</td>\n",
       "      <td>111.239998</td>\n",
       "      <td>107.970001</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>921500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851234</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WAT</td>\n",
       "      <td>135.240005</td>\n",
       "      <td>134.389999</td>\n",
       "      <td>133.710007</td>\n",
       "      <td>135.300003</td>\n",
       "      <td>464200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851235</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WBA</td>\n",
       "      <td>83.459999</td>\n",
       "      <td>82.760002</td>\n",
       "      <td>82.419998</td>\n",
       "      <td>83.620003</td>\n",
       "      <td>3343200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851236</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WDC</td>\n",
       "      <td>68.550003</td>\n",
       "      <td>67.949997</td>\n",
       "      <td>67.610001</td>\n",
       "      <td>69.400002</td>\n",
       "      <td>2824100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851237</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WEC</td>\n",
       "      <td>58.980000</td>\n",
       "      <td>58.650002</td>\n",
       "      <td>58.419998</td>\n",
       "      <td>59.119999</td>\n",
       "      <td>1221800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851238</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WFC</td>\n",
       "      <td>54.889999</td>\n",
       "      <td>55.110001</td>\n",
       "      <td>54.790001</td>\n",
       "      <td>55.360001</td>\n",
       "      <td>15095500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851239</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WFM</td>\n",
       "      <td>31.059999</td>\n",
       "      <td>30.760000</td>\n",
       "      <td>30.670000</td>\n",
       "      <td>31.299999</td>\n",
       "      <td>2707500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851240</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WHR</td>\n",
       "      <td>183.800003</td>\n",
       "      <td>181.770004</td>\n",
       "      <td>180.869995</td>\n",
       "      <td>184.289993</td>\n",
       "      <td>458200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851241</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WM</td>\n",
       "      <td>71.269997</td>\n",
       "      <td>70.910004</td>\n",
       "      <td>70.750000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>1230600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851242</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WMB</td>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.139999</td>\n",
       "      <td>30.889999</td>\n",
       "      <td>31.650000</td>\n",
       "      <td>3980300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851243</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WMT</td>\n",
       "      <td>69.120003</td>\n",
       "      <td>69.120003</td>\n",
       "      <td>68.830002</td>\n",
       "      <td>69.430000</td>\n",
       "      <td>6872000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851244</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WRK</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>50.529999</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>811200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851245</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WU</td>\n",
       "      <td>21.840000</td>\n",
       "      <td>21.719999</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>2538900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851246</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WY</td>\n",
       "      <td>30.450001</td>\n",
       "      <td>30.090000</td>\n",
       "      <td>29.950001</td>\n",
       "      <td>30.450001</td>\n",
       "      <td>2825300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851247</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WYN</td>\n",
       "      <td>76.849998</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>76.180000</td>\n",
       "      <td>76.970001</td>\n",
       "      <td>524600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851248</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WYNN</td>\n",
       "      <td>87.099998</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>85.570000</td>\n",
       "      <td>87.449997</td>\n",
       "      <td>1888500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851249</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XEC</td>\n",
       "      <td>136.520004</td>\n",
       "      <td>135.899994</td>\n",
       "      <td>135.309998</td>\n",
       "      <td>137.559998</td>\n",
       "      <td>466100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851250</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XEL</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.700001</td>\n",
       "      <td>40.560001</td>\n",
       "      <td>41.070000</td>\n",
       "      <td>1887600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851251</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XL</td>\n",
       "      <td>37.360001</td>\n",
       "      <td>37.259998</td>\n",
       "      <td>37.060001</td>\n",
       "      <td>37.419998</td>\n",
       "      <td>959200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851252</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XLNX</td>\n",
       "      <td>61.090000</td>\n",
       "      <td>60.369999</td>\n",
       "      <td>60.020000</td>\n",
       "      <td>61.480000</td>\n",
       "      <td>2111700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851253</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>90.029999</td>\n",
       "      <td>90.260002</td>\n",
       "      <td>90.010002</td>\n",
       "      <td>90.699997</td>\n",
       "      <td>9117800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851254</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XRAY</td>\n",
       "      <td>58.290001</td>\n",
       "      <td>57.730000</td>\n",
       "      <td>57.540001</td>\n",
       "      <td>58.360001</td>\n",
       "      <td>949200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851255</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XRX</td>\n",
       "      <td>8.720000</td>\n",
       "      <td>8.730000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>11250400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851256</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XYL</td>\n",
       "      <td>49.980000</td>\n",
       "      <td>49.520000</td>\n",
       "      <td>49.360001</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>646200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851257</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>YHOO</td>\n",
       "      <td>38.720001</td>\n",
       "      <td>38.669998</td>\n",
       "      <td>38.430000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>6431600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851258</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>YUM</td>\n",
       "      <td>63.930000</td>\n",
       "      <td>63.330002</td>\n",
       "      <td>63.160000</td>\n",
       "      <td>63.939999</td>\n",
       "      <td>1887100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851259</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZBH</td>\n",
       "      <td>103.309998</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>102.849998</td>\n",
       "      <td>103.930000</td>\n",
       "      <td>973800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851260</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>43.070000</td>\n",
       "      <td>43.040001</td>\n",
       "      <td>42.689999</td>\n",
       "      <td>43.310001</td>\n",
       "      <td>1938100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851261</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>53.639999</td>\n",
       "      <td>53.529999</td>\n",
       "      <td>53.270000</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>1701200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851262</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>AIV</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>45.450001</td>\n",
       "      <td>44.410000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>1380900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851263</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>FTV</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>53.630001</td>\n",
       "      <td>53.389999</td>\n",
       "      <td>54.480000</td>\n",
       "      <td>705100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>851264 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date symbol        open       close         low  \\\n",
       "0       2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998   \n",
       "1       2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002   \n",
       "2       2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000   \n",
       "3       2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000   \n",
       "4       2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996   \n",
       "5       2016-01-12 00:00:00   WLTW  115.510002  115.550003  114.500000   \n",
       "6       2016-01-13 00:00:00   WLTW  116.459999  112.849998  112.589996   \n",
       "7       2016-01-14 00:00:00   WLTW  113.510002  114.379997  110.050003   \n",
       "8       2016-01-15 00:00:00   WLTW  113.330002  112.529999  111.919998   \n",
       "9       2016-01-19 00:00:00   WLTW  113.660004  110.379997  109.870003   \n",
       "10      2016-01-20 00:00:00   WLTW  109.059998  109.300003  108.320000   \n",
       "11      2016-01-21 00:00:00   WLTW  109.730003  110.000000  108.320000   \n",
       "12      2016-01-22 00:00:00   WLTW  111.879997  111.949997  110.190002   \n",
       "13      2016-01-25 00:00:00   WLTW  111.320000  110.120003  110.000000   \n",
       "14      2016-01-26 00:00:00   WLTW  110.419998  111.000000  107.300003   \n",
       "15      2016-01-27 00:00:00   WLTW  110.769997  110.709999  109.019997   \n",
       "16      2016-01-28 00:00:00   WLTW  110.900002  112.580002  109.900002   \n",
       "17      2016-01-29 00:00:00   WLTW  113.349998  114.470001  111.669998   \n",
       "18      2016-02-01 00:00:00   WLTW  114.000000  114.500000  112.900002   \n",
       "19      2016-02-02 00:00:00   WLTW  113.250000  110.559998  109.750000   \n",
       "20      2016-02-03 00:00:00   WLTW  113.379997  114.050003  109.639999   \n",
       "21      2016-02-04 00:00:00   WLTW  114.080002  115.709999  114.080002   \n",
       "22      2016-02-05 00:00:00   WLTW  115.120003  114.019997  109.709999   \n",
       "23      2016-02-08 00:00:00   WLTW  113.300003  111.160004  110.459999   \n",
       "24      2016-02-09 00:00:00   WLTW  111.169998  110.650002  109.639999   \n",
       "25      2016-02-10 00:00:00   WLTW  106.730003  107.519997  106.360001   \n",
       "26      2016-02-11 00:00:00   WLTW  105.629997  107.129997  104.110001   \n",
       "27      2016-02-12 00:00:00   WLTW  108.559998  107.839996  107.070000   \n",
       "28      2016-02-16 00:00:00   WLTW  109.110001  110.769997  107.010002   \n",
       "29      2016-02-17 00:00:00   WLTW  110.830002  111.239998  107.970001   \n",
       "...                     ...    ...         ...         ...         ...   \n",
       "851234           2016-12-30    WAT  135.240005  134.389999  133.710007   \n",
       "851235           2016-12-30    WBA   83.459999   82.760002   82.419998   \n",
       "851236           2016-12-30    WDC   68.550003   67.949997   67.610001   \n",
       "851237           2016-12-30    WEC   58.980000   58.650002   58.419998   \n",
       "851238           2016-12-30    WFC   54.889999   55.110001   54.790001   \n",
       "851239           2016-12-30    WFM   31.059999   30.760000   30.670000   \n",
       "851240           2016-12-30    WHR  183.800003  181.770004  180.869995   \n",
       "851241           2016-12-30     WM   71.269997   70.910004   70.750000   \n",
       "851242           2016-12-30    WMB   30.940001   31.139999   30.889999   \n",
       "851243           2016-12-30    WMT   69.120003   69.120003   68.830002   \n",
       "851244           2016-12-30    WRK   51.840000   50.770000   50.529999   \n",
       "851245           2016-12-30     WU   21.840000   21.719999   21.600000   \n",
       "851246           2016-12-30     WY   30.450001   30.090000   29.950001   \n",
       "851247           2016-12-30    WYN   76.849998   76.370003   76.180000   \n",
       "851248           2016-12-30   WYNN   87.099998   86.510002   85.570000   \n",
       "851249           2016-12-30    XEC  136.520004  135.899994  135.309998   \n",
       "851250           2016-12-30    XEL   41.000000   40.700001   40.560001   \n",
       "851251           2016-12-30     XL   37.360001   37.259998   37.060001   \n",
       "851252           2016-12-30   XLNX   61.090000   60.369999   60.020000   \n",
       "851253           2016-12-30    XOM   90.029999   90.260002   90.010002   \n",
       "851254           2016-12-30   XRAY   58.290001   57.730000   57.540001   \n",
       "851255           2016-12-30    XRX    8.720000    8.730000    8.700000   \n",
       "851256           2016-12-30    XYL   49.980000   49.520000   49.360001   \n",
       "851257           2016-12-30   YHOO   38.720001   38.669998   38.430000   \n",
       "851258           2016-12-30    YUM   63.930000   63.330002   63.160000   \n",
       "851259           2016-12-30    ZBH  103.309998  103.199997  102.849998   \n",
       "851260           2016-12-30   ZION   43.070000   43.040001   42.689999   \n",
       "851261           2016-12-30    ZTS   53.639999   53.529999   53.270000   \n",
       "851262  2016-12-30 00:00:00    AIV   44.730000   45.450001   44.410000   \n",
       "851263  2016-12-30 00:00:00    FTV   54.200001   53.630001   53.389999   \n",
       "\n",
       "              high      volume  \n",
       "0       126.250000   2163600.0  \n",
       "1       125.540001   2386400.0  \n",
       "2       119.739998   2489500.0  \n",
       "3       117.440002   2006300.0  \n",
       "4       117.330002   1408600.0  \n",
       "5       116.059998   1098000.0  \n",
       "6       117.070000    949600.0  \n",
       "7       115.029999    785300.0  \n",
       "8       114.879997   1093700.0  \n",
       "9       115.870003   1523500.0  \n",
       "10      111.599998   1653900.0  \n",
       "11      110.580002    944300.0  \n",
       "12      112.949997    744900.0  \n",
       "13      114.629997    703800.0  \n",
       "14      111.400002    563100.0  \n",
       "15      112.570000    896100.0  \n",
       "16      112.970001    680400.0  \n",
       "17      114.589996    749900.0  \n",
       "18      114.849998    574200.0  \n",
       "19      113.860001    694800.0  \n",
       "20      114.639999    896300.0  \n",
       "21      116.320000    956300.0  \n",
       "22      116.489998    997100.0  \n",
       "23      113.300003   1200500.0  \n",
       "24      112.110001   1725200.0  \n",
       "25      112.110001   1946000.0  \n",
       "26      109.260002   1319500.0  \n",
       "27      109.430000    922400.0  \n",
       "28      111.300003   1185100.0  \n",
       "29      112.110001    921500.0  \n",
       "...            ...         ...  \n",
       "851234  135.300003    464200.0  \n",
       "851235   83.620003   3343200.0  \n",
       "851236   69.400002   2824100.0  \n",
       "851237   59.119999   1221800.0  \n",
       "851238   55.360001  15095500.0  \n",
       "851239   31.299999   2707500.0  \n",
       "851240  184.289993    458200.0  \n",
       "851241   71.500000   1230600.0  \n",
       "851242   31.650000   3980300.0  \n",
       "851243   69.430000   6872000.0  \n",
       "851244   51.840000    811200.0  \n",
       "851245   21.900000   2538900.0  \n",
       "851246   30.450001   2825300.0  \n",
       "851247   76.970001    524600.0  \n",
       "851248   87.449997   1888500.0  \n",
       "851249  137.559998    466100.0  \n",
       "851250   41.070000   1887600.0  \n",
       "851251   37.419998    959200.0  \n",
       "851252   61.480000   2111700.0  \n",
       "851253   90.699997   9117800.0  \n",
       "851254   58.360001    949200.0  \n",
       "851255    8.800000  11250400.0  \n",
       "851256   50.000000    646200.0  \n",
       "851257   39.000000   6431600.0  \n",
       "851258   63.939999   1887100.0  \n",
       "851259  103.930000    973800.0  \n",
       "851260   43.310001   1938100.0  \n",
       "851261   53.740002   1701200.0  \n",
       "851262   45.590000   1380900.0  \n",
       "851263   54.480000    705100.0  \n",
       "\n",
       "[851264 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell to upload file if you are using google colab\n",
    "# from google.colab import files\n",
    "# files.upload()\n",
    "\n",
    "df = pd.read_csv('pricesandiris/prices.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlLKVPVH_BCT"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J4BlzVA_gZd"
   },
   "source": [
    "### Drop columnns\n",
    "- drop \"date\" and \"symbol\" column from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKEK8aEE_Csx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>115.510002</td>\n",
       "      <td>115.550003</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>116.059998</td>\n",
       "      <td>1098000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>116.459999</td>\n",
       "      <td>112.849998</td>\n",
       "      <td>112.589996</td>\n",
       "      <td>117.070000</td>\n",
       "      <td>949600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113.510002</td>\n",
       "      <td>114.379997</td>\n",
       "      <td>110.050003</td>\n",
       "      <td>115.029999</td>\n",
       "      <td>785300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>113.330002</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>111.919998</td>\n",
       "      <td>114.879997</td>\n",
       "      <td>1093700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>113.660004</td>\n",
       "      <td>110.379997</td>\n",
       "      <td>109.870003</td>\n",
       "      <td>115.870003</td>\n",
       "      <td>1523500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>109.059998</td>\n",
       "      <td>109.300003</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>111.599998</td>\n",
       "      <td>1653900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>109.730003</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>110.580002</td>\n",
       "      <td>944300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>111.879997</td>\n",
       "      <td>111.949997</td>\n",
       "      <td>110.190002</td>\n",
       "      <td>112.949997</td>\n",
       "      <td>744900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>111.320000</td>\n",
       "      <td>110.120003</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>114.629997</td>\n",
       "      <td>703800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>110.419998</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>107.300003</td>\n",
       "      <td>111.400002</td>\n",
       "      <td>563100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>110.769997</td>\n",
       "      <td>110.709999</td>\n",
       "      <td>109.019997</td>\n",
       "      <td>112.570000</td>\n",
       "      <td>896100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>110.900002</td>\n",
       "      <td>112.580002</td>\n",
       "      <td>109.900002</td>\n",
       "      <td>112.970001</td>\n",
       "      <td>680400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>113.349998</td>\n",
       "      <td>114.470001</td>\n",
       "      <td>111.669998</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>749900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>112.900002</td>\n",
       "      <td>114.849998</td>\n",
       "      <td>574200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>113.250000</td>\n",
       "      <td>110.559998</td>\n",
       "      <td>109.750000</td>\n",
       "      <td>113.860001</td>\n",
       "      <td>694800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>113.379997</td>\n",
       "      <td>114.050003</td>\n",
       "      <td>109.639999</td>\n",
       "      <td>114.639999</td>\n",
       "      <td>896300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>114.080002</td>\n",
       "      <td>115.709999</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>116.320000</td>\n",
       "      <td>956300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>115.120003</td>\n",
       "      <td>114.019997</td>\n",
       "      <td>109.709999</td>\n",
       "      <td>116.489998</td>\n",
       "      <td>997100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>113.300003</td>\n",
       "      <td>111.160004</td>\n",
       "      <td>110.459999</td>\n",
       "      <td>113.300003</td>\n",
       "      <td>1200500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>111.169998</td>\n",
       "      <td>110.650002</td>\n",
       "      <td>109.639999</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>1725200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>106.730003</td>\n",
       "      <td>107.519997</td>\n",
       "      <td>106.360001</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>1946000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>105.629997</td>\n",
       "      <td>107.129997</td>\n",
       "      <td>104.110001</td>\n",
       "      <td>109.260002</td>\n",
       "      <td>1319500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>108.559998</td>\n",
       "      <td>107.839996</td>\n",
       "      <td>107.070000</td>\n",
       "      <td>109.430000</td>\n",
       "      <td>922400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>109.110001</td>\n",
       "      <td>110.769997</td>\n",
       "      <td>107.010002</td>\n",
       "      <td>111.300003</td>\n",
       "      <td>1185100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>110.830002</td>\n",
       "      <td>111.239998</td>\n",
       "      <td>107.970001</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>921500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851234</th>\n",
       "      <td>135.240005</td>\n",
       "      <td>134.389999</td>\n",
       "      <td>133.710007</td>\n",
       "      <td>135.300003</td>\n",
       "      <td>464200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851235</th>\n",
       "      <td>83.459999</td>\n",
       "      <td>82.760002</td>\n",
       "      <td>82.419998</td>\n",
       "      <td>83.620003</td>\n",
       "      <td>3343200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851236</th>\n",
       "      <td>68.550003</td>\n",
       "      <td>67.949997</td>\n",
       "      <td>67.610001</td>\n",
       "      <td>69.400002</td>\n",
       "      <td>2824100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851237</th>\n",
       "      <td>58.980000</td>\n",
       "      <td>58.650002</td>\n",
       "      <td>58.419998</td>\n",
       "      <td>59.119999</td>\n",
       "      <td>1221800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851238</th>\n",
       "      <td>54.889999</td>\n",
       "      <td>55.110001</td>\n",
       "      <td>54.790001</td>\n",
       "      <td>55.360001</td>\n",
       "      <td>15095500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851239</th>\n",
       "      <td>31.059999</td>\n",
       "      <td>30.760000</td>\n",
       "      <td>30.670000</td>\n",
       "      <td>31.299999</td>\n",
       "      <td>2707500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851240</th>\n",
       "      <td>183.800003</td>\n",
       "      <td>181.770004</td>\n",
       "      <td>180.869995</td>\n",
       "      <td>184.289993</td>\n",
       "      <td>458200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851241</th>\n",
       "      <td>71.269997</td>\n",
       "      <td>70.910004</td>\n",
       "      <td>70.750000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>1230600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851242</th>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.139999</td>\n",
       "      <td>30.889999</td>\n",
       "      <td>31.650000</td>\n",
       "      <td>3980300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851243</th>\n",
       "      <td>69.120003</td>\n",
       "      <td>69.120003</td>\n",
       "      <td>68.830002</td>\n",
       "      <td>69.430000</td>\n",
       "      <td>6872000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851244</th>\n",
       "      <td>51.840000</td>\n",
       "      <td>50.770000</td>\n",
       "      <td>50.529999</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>811200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851245</th>\n",
       "      <td>21.840000</td>\n",
       "      <td>21.719999</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>2538900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851246</th>\n",
       "      <td>30.450001</td>\n",
       "      <td>30.090000</td>\n",
       "      <td>29.950001</td>\n",
       "      <td>30.450001</td>\n",
       "      <td>2825300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851247</th>\n",
       "      <td>76.849998</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>76.180000</td>\n",
       "      <td>76.970001</td>\n",
       "      <td>524600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851248</th>\n",
       "      <td>87.099998</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>85.570000</td>\n",
       "      <td>87.449997</td>\n",
       "      <td>1888500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851249</th>\n",
       "      <td>136.520004</td>\n",
       "      <td>135.899994</td>\n",
       "      <td>135.309998</td>\n",
       "      <td>137.559998</td>\n",
       "      <td>466100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851250</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.700001</td>\n",
       "      <td>40.560001</td>\n",
       "      <td>41.070000</td>\n",
       "      <td>1887600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851251</th>\n",
       "      <td>37.360001</td>\n",
       "      <td>37.259998</td>\n",
       "      <td>37.060001</td>\n",
       "      <td>37.419998</td>\n",
       "      <td>959200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851252</th>\n",
       "      <td>61.090000</td>\n",
       "      <td>60.369999</td>\n",
       "      <td>60.020000</td>\n",
       "      <td>61.480000</td>\n",
       "      <td>2111700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851253</th>\n",
       "      <td>90.029999</td>\n",
       "      <td>90.260002</td>\n",
       "      <td>90.010002</td>\n",
       "      <td>90.699997</td>\n",
       "      <td>9117800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851254</th>\n",
       "      <td>58.290001</td>\n",
       "      <td>57.730000</td>\n",
       "      <td>57.540001</td>\n",
       "      <td>58.360001</td>\n",
       "      <td>949200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851255</th>\n",
       "      <td>8.720000</td>\n",
       "      <td>8.730000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>11250400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851256</th>\n",
       "      <td>49.980000</td>\n",
       "      <td>49.520000</td>\n",
       "      <td>49.360001</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>646200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851257</th>\n",
       "      <td>38.720001</td>\n",
       "      <td>38.669998</td>\n",
       "      <td>38.430000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>6431600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851258</th>\n",
       "      <td>63.930000</td>\n",
       "      <td>63.330002</td>\n",
       "      <td>63.160000</td>\n",
       "      <td>63.939999</td>\n",
       "      <td>1887100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851259</th>\n",
       "      <td>103.309998</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>102.849998</td>\n",
       "      <td>103.930000</td>\n",
       "      <td>973800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851260</th>\n",
       "      <td>43.070000</td>\n",
       "      <td>43.040001</td>\n",
       "      <td>42.689999</td>\n",
       "      <td>43.310001</td>\n",
       "      <td>1938100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851261</th>\n",
       "      <td>53.639999</td>\n",
       "      <td>53.529999</td>\n",
       "      <td>53.270000</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>1701200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851262</th>\n",
       "      <td>44.730000</td>\n",
       "      <td>45.450001</td>\n",
       "      <td>44.410000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>1380900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851263</th>\n",
       "      <td>54.200001</td>\n",
       "      <td>53.630001</td>\n",
       "      <td>53.389999</td>\n",
       "      <td>54.480000</td>\n",
       "      <td>705100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>851264 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              open       close         low        high      volume\n",
       "0       123.430000  125.839996  122.309998  126.250000   2163600.0\n",
       "1       125.239998  119.980003  119.940002  125.540001   2386400.0\n",
       "2       116.379997  114.949997  114.930000  119.739998   2489500.0\n",
       "3       115.480003  116.620003  113.500000  117.440002   2006300.0\n",
       "4       117.010002  114.970001  114.089996  117.330002   1408600.0\n",
       "5       115.510002  115.550003  114.500000  116.059998   1098000.0\n",
       "6       116.459999  112.849998  112.589996  117.070000    949600.0\n",
       "7       113.510002  114.379997  110.050003  115.029999    785300.0\n",
       "8       113.330002  112.529999  111.919998  114.879997   1093700.0\n",
       "9       113.660004  110.379997  109.870003  115.870003   1523500.0\n",
       "10      109.059998  109.300003  108.320000  111.599998   1653900.0\n",
       "11      109.730003  110.000000  108.320000  110.580002    944300.0\n",
       "12      111.879997  111.949997  110.190002  112.949997    744900.0\n",
       "13      111.320000  110.120003  110.000000  114.629997    703800.0\n",
       "14      110.419998  111.000000  107.300003  111.400002    563100.0\n",
       "15      110.769997  110.709999  109.019997  112.570000    896100.0\n",
       "16      110.900002  112.580002  109.900002  112.970001    680400.0\n",
       "17      113.349998  114.470001  111.669998  114.589996    749900.0\n",
       "18      114.000000  114.500000  112.900002  114.849998    574200.0\n",
       "19      113.250000  110.559998  109.750000  113.860001    694800.0\n",
       "20      113.379997  114.050003  109.639999  114.639999    896300.0\n",
       "21      114.080002  115.709999  114.080002  116.320000    956300.0\n",
       "22      115.120003  114.019997  109.709999  116.489998    997100.0\n",
       "23      113.300003  111.160004  110.459999  113.300003   1200500.0\n",
       "24      111.169998  110.650002  109.639999  112.110001   1725200.0\n",
       "25      106.730003  107.519997  106.360001  112.110001   1946000.0\n",
       "26      105.629997  107.129997  104.110001  109.260002   1319500.0\n",
       "27      108.559998  107.839996  107.070000  109.430000    922400.0\n",
       "28      109.110001  110.769997  107.010002  111.300003   1185100.0\n",
       "29      110.830002  111.239998  107.970001  112.110001    921500.0\n",
       "...            ...         ...         ...         ...         ...\n",
       "851234  135.240005  134.389999  133.710007  135.300003    464200.0\n",
       "851235   83.459999   82.760002   82.419998   83.620003   3343200.0\n",
       "851236   68.550003   67.949997   67.610001   69.400002   2824100.0\n",
       "851237   58.980000   58.650002   58.419998   59.119999   1221800.0\n",
       "851238   54.889999   55.110001   54.790001   55.360001  15095500.0\n",
       "851239   31.059999   30.760000   30.670000   31.299999   2707500.0\n",
       "851240  183.800003  181.770004  180.869995  184.289993    458200.0\n",
       "851241   71.269997   70.910004   70.750000   71.500000   1230600.0\n",
       "851242   30.940001   31.139999   30.889999   31.650000   3980300.0\n",
       "851243   69.120003   69.120003   68.830002   69.430000   6872000.0\n",
       "851244   51.840000   50.770000   50.529999   51.840000    811200.0\n",
       "851245   21.840000   21.719999   21.600000   21.900000   2538900.0\n",
       "851246   30.450001   30.090000   29.950001   30.450001   2825300.0\n",
       "851247   76.849998   76.370003   76.180000   76.970001    524600.0\n",
       "851248   87.099998   86.510002   85.570000   87.449997   1888500.0\n",
       "851249  136.520004  135.899994  135.309998  137.559998    466100.0\n",
       "851250   41.000000   40.700001   40.560001   41.070000   1887600.0\n",
       "851251   37.360001   37.259998   37.060001   37.419998    959200.0\n",
       "851252   61.090000   60.369999   60.020000   61.480000   2111700.0\n",
       "851253   90.029999   90.260002   90.010002   90.699997   9117800.0\n",
       "851254   58.290001   57.730000   57.540001   58.360001    949200.0\n",
       "851255    8.720000    8.730000    8.700000    8.800000  11250400.0\n",
       "851256   49.980000   49.520000   49.360001   50.000000    646200.0\n",
       "851257   38.720001   38.669998   38.430000   39.000000   6431600.0\n",
       "851258   63.930000   63.330002   63.160000   63.939999   1887100.0\n",
       "851259  103.309998  103.199997  102.849998  103.930000    973800.0\n",
       "851260   43.070000   43.040001   42.689999   43.310001   1938100.0\n",
       "851261   53.639999   53.529999   53.270000   53.740002   1701200.0\n",
       "851262   44.730000   45.450001   44.410000   45.590000   1380900.0\n",
       "851263   54.200001   53.630001   53.389999   54.480000    705100.0\n",
       "\n",
       "[851264 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop = df.drop(['date','symbol'], axis=1)\n",
    "df_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTPhO6v-AiZt"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsZXmF3NAkna"
   },
   "source": [
    "### Take initial rows\n",
    "- Take first 1000 rows from the data\n",
    "- This step is done to make the execution faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKs04iIHAjxN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>115.510002</td>\n",
       "      <td>115.550003</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>116.059998</td>\n",
       "      <td>1098000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>116.459999</td>\n",
       "      <td>112.849998</td>\n",
       "      <td>112.589996</td>\n",
       "      <td>117.070000</td>\n",
       "      <td>949600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113.510002</td>\n",
       "      <td>114.379997</td>\n",
       "      <td>110.050003</td>\n",
       "      <td>115.029999</td>\n",
       "      <td>785300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>113.330002</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>111.919998</td>\n",
       "      <td>114.879997</td>\n",
       "      <td>1093700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>113.660004</td>\n",
       "      <td>110.379997</td>\n",
       "      <td>109.870003</td>\n",
       "      <td>115.870003</td>\n",
       "      <td>1523500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>109.059998</td>\n",
       "      <td>109.300003</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>111.599998</td>\n",
       "      <td>1653900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>109.730003</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>110.580002</td>\n",
       "      <td>944300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>111.879997</td>\n",
       "      <td>111.949997</td>\n",
       "      <td>110.190002</td>\n",
       "      <td>112.949997</td>\n",
       "      <td>744900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>111.320000</td>\n",
       "      <td>110.120003</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>114.629997</td>\n",
       "      <td>703800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>110.419998</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>107.300003</td>\n",
       "      <td>111.400002</td>\n",
       "      <td>563100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>110.769997</td>\n",
       "      <td>110.709999</td>\n",
       "      <td>109.019997</td>\n",
       "      <td>112.570000</td>\n",
       "      <td>896100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>110.900002</td>\n",
       "      <td>112.580002</td>\n",
       "      <td>109.900002</td>\n",
       "      <td>112.970001</td>\n",
       "      <td>680400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>113.349998</td>\n",
       "      <td>114.470001</td>\n",
       "      <td>111.669998</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>749900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>112.900002</td>\n",
       "      <td>114.849998</td>\n",
       "      <td>574200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>113.250000</td>\n",
       "      <td>110.559998</td>\n",
       "      <td>109.750000</td>\n",
       "      <td>113.860001</td>\n",
       "      <td>694800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>113.379997</td>\n",
       "      <td>114.050003</td>\n",
       "      <td>109.639999</td>\n",
       "      <td>114.639999</td>\n",
       "      <td>896300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>114.080002</td>\n",
       "      <td>115.709999</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>116.320000</td>\n",
       "      <td>956300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>115.120003</td>\n",
       "      <td>114.019997</td>\n",
       "      <td>109.709999</td>\n",
       "      <td>116.489998</td>\n",
       "      <td>997100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>113.300003</td>\n",
       "      <td>111.160004</td>\n",
       "      <td>110.459999</td>\n",
       "      <td>113.300003</td>\n",
       "      <td>1200500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>111.169998</td>\n",
       "      <td>110.650002</td>\n",
       "      <td>109.639999</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>1725200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>106.730003</td>\n",
       "      <td>107.519997</td>\n",
       "      <td>106.360001</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>1946000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>105.629997</td>\n",
       "      <td>107.129997</td>\n",
       "      <td>104.110001</td>\n",
       "      <td>109.260002</td>\n",
       "      <td>1319500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>108.559998</td>\n",
       "      <td>107.839996</td>\n",
       "      <td>107.070000</td>\n",
       "      <td>109.430000</td>\n",
       "      <td>922400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>109.110001</td>\n",
       "      <td>110.769997</td>\n",
       "      <td>107.010002</td>\n",
       "      <td>111.300003</td>\n",
       "      <td>1185100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>110.830002</td>\n",
       "      <td>111.239998</td>\n",
       "      <td>107.970001</td>\n",
       "      <td>112.110001</td>\n",
       "      <td>921500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>19.240000</td>\n",
       "      <td>18.680000</td>\n",
       "      <td>18.530001</td>\n",
       "      <td>19.430000</td>\n",
       "      <td>9097500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>20.520000</td>\n",
       "      <td>20.290001</td>\n",
       "      <td>19.709999</td>\n",
       "      <td>20.549999</td>\n",
       "      <td>2242100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>12.950000</td>\n",
       "      <td>13.550000</td>\n",
       "      <td>12.720000</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>6205100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>76.059998</td>\n",
       "      <td>75.440002</td>\n",
       "      <td>75.349998</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>864600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.830000</td>\n",
       "      <td>19.480000</td>\n",
       "      <td>19.830000</td>\n",
       "      <td>709200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>87.370003</td>\n",
       "      <td>87.139999</td>\n",
       "      <td>88.239998</td>\n",
       "      <td>1129600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>31.010000</td>\n",
       "      <td>30.959999</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>31.120001</td>\n",
       "      <td>3449300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>35.900002</td>\n",
       "      <td>35.189999</td>\n",
       "      <td>34.930000</td>\n",
       "      <td>35.910000</td>\n",
       "      <td>7517100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>76.620003</td>\n",
       "      <td>77.650002</td>\n",
       "      <td>76.550003</td>\n",
       "      <td>77.790001</td>\n",
       "      <td>2356500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>25.799999</td>\n",
       "      <td>26.420000</td>\n",
       "      <td>25.719999</td>\n",
       "      <td>26.610001</td>\n",
       "      <td>4839200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>30.370001</td>\n",
       "      <td>31.059999</td>\n",
       "      <td>30.309999</td>\n",
       "      <td>31.110001</td>\n",
       "      <td>3684600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>23.120001</td>\n",
       "      <td>22.920000</td>\n",
       "      <td>22.740000</td>\n",
       "      <td>23.129999</td>\n",
       "      <td>14428400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>39.790001</td>\n",
       "      <td>39.610001</td>\n",
       "      <td>39.090000</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>1463300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>24.669997</td>\n",
       "      <td>25.140004</td>\n",
       "      <td>24.669997</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>749600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>11.250000</td>\n",
       "      <td>11.770000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>11.790000</td>\n",
       "      <td>13355100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1.630000</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>11538300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>17.020000</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>16.790001</td>\n",
       "      <td>17.209999</td>\n",
       "      <td>9931300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>257.840004</td>\n",
       "      <td>256.079998</td>\n",
       "      <td>253.050003</td>\n",
       "      <td>257.959995</td>\n",
       "      <td>12906000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>48.099998</td>\n",
       "      <td>48.150002</td>\n",
       "      <td>47.700001</td>\n",
       "      <td>48.369999</td>\n",
       "      <td>369900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>34.529999</td>\n",
       "      <td>34.430000</td>\n",
       "      <td>34.080002</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>2701000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>27.570000</td>\n",
       "      <td>27.790001</td>\n",
       "      <td>27.370000</td>\n",
       "      <td>27.919999</td>\n",
       "      <td>2627800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.420000</td>\n",
       "      <td>14.120000</td>\n",
       "      <td>14.430000</td>\n",
       "      <td>3258700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>19.900000</td>\n",
       "      <td>19.580000</td>\n",
       "      <td>19.330000</td>\n",
       "      <td>20.059999</td>\n",
       "      <td>5206100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>62.660000</td>\n",
       "      <td>62.299999</td>\n",
       "      <td>62.189999</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>7099000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>29.219999</td>\n",
       "      <td>28.719999</td>\n",
       "      <td>28.629999</td>\n",
       "      <td>29.309999</td>\n",
       "      <td>7795500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>63.310001</td>\n",
       "      <td>63.590000</td>\n",
       "      <td>63.240002</td>\n",
       "      <td>63.639999</td>\n",
       "      <td>2133200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>27.160000</td>\n",
       "      <td>26.990000</td>\n",
       "      <td>26.680000</td>\n",
       "      <td>27.299999</td>\n",
       "      <td>1982400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>28.320000</td>\n",
       "      <td>28.770000</td>\n",
       "      <td>28.010000</td>\n",
       "      <td>28.809999</td>\n",
       "      <td>37152800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.799999</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>44.810001</td>\n",
       "      <td>6568600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>36.080002</td>\n",
       "      <td>37.139999</td>\n",
       "      <td>36.009998</td>\n",
       "      <td>37.230000</td>\n",
       "      <td>5604300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           open       close         low        high      volume\n",
       "0    123.430000  125.839996  122.309998  126.250000   2163600.0\n",
       "1    125.239998  119.980003  119.940002  125.540001   2386400.0\n",
       "2    116.379997  114.949997  114.930000  119.739998   2489500.0\n",
       "3    115.480003  116.620003  113.500000  117.440002   2006300.0\n",
       "4    117.010002  114.970001  114.089996  117.330002   1408600.0\n",
       "5    115.510002  115.550003  114.500000  116.059998   1098000.0\n",
       "6    116.459999  112.849998  112.589996  117.070000    949600.0\n",
       "7    113.510002  114.379997  110.050003  115.029999    785300.0\n",
       "8    113.330002  112.529999  111.919998  114.879997   1093700.0\n",
       "9    113.660004  110.379997  109.870003  115.870003   1523500.0\n",
       "10   109.059998  109.300003  108.320000  111.599998   1653900.0\n",
       "11   109.730003  110.000000  108.320000  110.580002    944300.0\n",
       "12   111.879997  111.949997  110.190002  112.949997    744900.0\n",
       "13   111.320000  110.120003  110.000000  114.629997    703800.0\n",
       "14   110.419998  111.000000  107.300003  111.400002    563100.0\n",
       "15   110.769997  110.709999  109.019997  112.570000    896100.0\n",
       "16   110.900002  112.580002  109.900002  112.970001    680400.0\n",
       "17   113.349998  114.470001  111.669998  114.589996    749900.0\n",
       "18   114.000000  114.500000  112.900002  114.849998    574200.0\n",
       "19   113.250000  110.559998  109.750000  113.860001    694800.0\n",
       "20   113.379997  114.050003  109.639999  114.639999    896300.0\n",
       "21   114.080002  115.709999  114.080002  116.320000    956300.0\n",
       "22   115.120003  114.019997  109.709999  116.489998    997100.0\n",
       "23   113.300003  111.160004  110.459999  113.300003   1200500.0\n",
       "24   111.169998  110.650002  109.639999  112.110001   1725200.0\n",
       "25   106.730003  107.519997  106.360001  112.110001   1946000.0\n",
       "26   105.629997  107.129997  104.110001  109.260002   1319500.0\n",
       "27   108.559998  107.839996  107.070000  109.430000    922400.0\n",
       "28   109.110001  110.769997  107.010002  111.300003   1185100.0\n",
       "29   110.830002  111.239998  107.970001  112.110001    921500.0\n",
       "..          ...         ...         ...         ...         ...\n",
       "970   19.240000   18.680000   18.530001   19.430000   9097500.0\n",
       "971   20.520000   20.290001   19.709999   20.549999   2242100.0\n",
       "972   12.950000   13.550000   12.720000   13.600000   6205100.0\n",
       "973   76.059998   75.440002   75.349998   76.370003    864600.0\n",
       "974   19.620001   19.830000   19.480000   19.830000    709200.0\n",
       "975   88.000000   87.370003   87.139999   88.239998   1129600.0\n",
       "976   31.010000   30.959999   30.700001   31.120001   3449300.0\n",
       "977   35.900002   35.189999   34.930000   35.910000   7517100.0\n",
       "978   76.620003   77.650002   76.550003   77.790001   2356500.0\n",
       "979   25.799999   26.420000   25.719999   26.610001   4839200.0\n",
       "980   30.370001   31.059999   30.309999   31.110001   3684600.0\n",
       "981   23.120001   22.920000   22.740000   23.129999  14428400.0\n",
       "982   39.790001   39.610001   39.090000   39.799999   1463300.0\n",
       "983   24.669997   25.140004   24.669997   25.160000    749600.0\n",
       "984   11.250000   11.770000   11.200000   11.790000  13355100.0\n",
       "985    1.630000    1.650000    1.600000    1.660000  11538300.0\n",
       "986   17.020000   16.860001   16.790001   17.209999   9931300.0\n",
       "987  257.840004  256.079998  253.050003  257.959995  12906000.0\n",
       "988   48.099998   48.150002   47.700001   48.369999    369900.0\n",
       "989   34.529999   34.430000   34.080002   34.750000   2701000.0\n",
       "990   27.570000   27.790001   27.370000   27.919999   2627800.0\n",
       "991   14.200000   14.420000   14.120000   14.430000   3258700.0\n",
       "992   19.900000   19.580000   19.330000   20.059999   5206100.0\n",
       "993   62.660000   62.299999   62.189999   62.750000   7099000.0\n",
       "994   29.219999   28.719999   28.629999   29.309999   7795500.0\n",
       "995   63.310001   63.590000   63.240002   63.639999   2133200.0\n",
       "996   27.160000   26.990000   26.680000   27.299999   1982400.0\n",
       "997   28.320000   28.770000   28.010000   28.809999  37152800.0\n",
       "998   44.000000   44.799999   43.750000   44.810001   6568600.0\n",
       "999   36.080002   37.139999   36.009998   37.230000   5604300.0\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1000 = df_drop[0:1000]\n",
    "df_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vGtnapgBIJm"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8u_jlbABTip"
   },
   "source": [
    "### Get features and label from the dataset in separate variable\n",
    "- Take \"open\", \"close\", \"low\", \"high\" columns as features\n",
    "- Take \"volume\" column as label\n",
    "- Normalize label column by dividing it with 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQjCMzUXBJbg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "           open       close         low        high\n",
      "0    123.430000  125.839996  122.309998  126.250000\n",
      "1    125.239998  119.980003  119.940002  125.540001\n",
      "2    116.379997  114.949997  114.930000  119.739998\n",
      "3    115.480003  116.620003  113.500000  117.440002\n",
      "4    117.010002  114.970001  114.089996  117.330002\n",
      "5    115.510002  115.550003  114.500000  116.059998\n",
      "6    116.459999  112.849998  112.589996  117.070000\n",
      "7    113.510002  114.379997  110.050003  115.029999\n",
      "8    113.330002  112.529999  111.919998  114.879997\n",
      "9    113.660004  110.379997  109.870003  115.870003\n",
      "10   109.059998  109.300003  108.320000  111.599998\n",
      "11   109.730003  110.000000  108.320000  110.580002\n",
      "12   111.879997  111.949997  110.190002  112.949997\n",
      "13   111.320000  110.120003  110.000000  114.629997\n",
      "14   110.419998  111.000000  107.300003  111.400002\n",
      "15   110.769997  110.709999  109.019997  112.570000\n",
      "16   110.900002  112.580002  109.900002  112.970001\n",
      "17   113.349998  114.470001  111.669998  114.589996\n",
      "18   114.000000  114.500000  112.900002  114.849998\n",
      "19   113.250000  110.559998  109.750000  113.860001\n",
      "20   113.379997  114.050003  109.639999  114.639999\n",
      "21   114.080002  115.709999  114.080002  116.320000\n",
      "22   115.120003  114.019997  109.709999  116.489998\n",
      "23   113.300003  111.160004  110.459999  113.300003\n",
      "24   111.169998  110.650002  109.639999  112.110001\n",
      "25   106.730003  107.519997  106.360001  112.110001\n",
      "26   105.629997  107.129997  104.110001  109.260002\n",
      "27   108.559998  107.839996  107.070000  109.430000\n",
      "28   109.110001  110.769997  107.010002  111.300003\n",
      "29   110.830002  111.239998  107.970001  112.110001\n",
      "..          ...         ...         ...         ...\n",
      "970   19.240000   18.680000   18.530001   19.430000\n",
      "971   20.520000   20.290001   19.709999   20.549999\n",
      "972   12.950000   13.550000   12.720000   13.600000\n",
      "973   76.059998   75.440002   75.349998   76.370003\n",
      "974   19.620001   19.830000   19.480000   19.830000\n",
      "975   88.000000   87.370003   87.139999   88.239998\n",
      "976   31.010000   30.959999   30.700001   31.120001\n",
      "977   35.900002   35.189999   34.930000   35.910000\n",
      "978   76.620003   77.650002   76.550003   77.790001\n",
      "979   25.799999   26.420000   25.719999   26.610001\n",
      "980   30.370001   31.059999   30.309999   31.110001\n",
      "981   23.120001   22.920000   22.740000   23.129999\n",
      "982   39.790001   39.610001   39.090000   39.799999\n",
      "983   24.669997   25.140004   24.669997   25.160000\n",
      "984   11.250000   11.770000   11.200000   11.790000\n",
      "985    1.630000    1.650000    1.600000    1.660000\n",
      "986   17.020000   16.860001   16.790001   17.209999\n",
      "987  257.840004  256.079998  253.050003  257.959995\n",
      "988   48.099998   48.150002   47.700001   48.369999\n",
      "989   34.529999   34.430000   34.080002   34.750000\n",
      "990   27.570000   27.790001   27.370000   27.919999\n",
      "991   14.200000   14.420000   14.120000   14.430000\n",
      "992   19.900000   19.580000   19.330000   20.059999\n",
      "993   62.660000   62.299999   62.189999   62.750000\n",
      "994   29.219999   28.719999   28.629999   29.309999\n",
      "995   63.310001   63.590000   63.240002   63.639999\n",
      "996   27.160000   26.990000   26.680000   27.299999\n",
      "997   28.320000   28.770000   28.010000   28.809999\n",
      "998   44.000000   44.799999   43.750000   44.810001\n",
      "999   36.080002   37.139999   36.009998   37.230000\n",
      "\n",
      "[1000 rows x 4 columns]\n",
      "[0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05]\n"
     ]
    }
   ],
   "source": [
    "X_1000 = df_1000.iloc[:,0:4]\n",
    "Y_1000 = df_1000.iloc[:,4:]\n",
    "Y_1000_norm = Y/100000\n",
    "print(type(X_1000))\n",
    "print(type(Y_1000_norm))\n",
    "print(X_1000)\n",
    "print(Y_1000_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTAKzlxZBz0z"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfY8Km1Zzyt2"
   },
   "source": [
    "### Convert data\n",
    "- Convert features and labels to numpy array\n",
    "- Convert their data type to \"float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ko7nnQVbYENh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'numpy.ndarray'> [[123.43 125.84 122.31 126.25]\n",
      " [125.24 119.98 119.94 125.54]\n",
      " [116.38 114.95 114.93 119.74]\n",
      " ...\n",
      " [ 28.32  28.77  28.01  28.81]\n",
      " [ 44.    44.8   43.75  44.81]\n",
      " [ 36.08  37.14  36.01  37.23]]\n",
      "data_type: float32 [0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05 1.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05\n",
      " 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05 2.e-05]\n"
     ]
    }
   ],
   "source": [
    "X_np_array = X_1000.to_numpy().astype(np.float32)\n",
    "# Y_np_array = Y_1000_norm.to_numpy().astype(np.float32)\n",
    "\n",
    "# X_np_array = X_1000.astype(np.float32)\n",
    "Y_np_array = Y_1000_norm.astype(np.float32)\n",
    "\n",
    "print(\"type:\",type(X_np_array),X_np_array)\n",
    "print(\"data_type:\",Y_np_array.dtype, Y_np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TWpN0nVTpUx"
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ1FKEs-4btX"
   },
   "source": [
    "### Normalize data\n",
    "- Normalize features\n",
    "- Use tf.math.l2_normalize to normalize features\n",
    "- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0Tfe00X78wB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3442, shape=(1000, 4), dtype=float32, numpy=\n",
       "array([[0.02202894, 0.02245906, 0.02182905, 0.02253223],\n",
       "       [0.02235197, 0.02141321, 0.02140607, 0.02240552],\n",
       "       [0.0207707 , 0.02051548, 0.02051192, 0.02137037],\n",
       "       ...,\n",
       "       [0.00505436, 0.00513467, 0.00499903, 0.00514181],\n",
       "       [0.00785282, 0.0079956 , 0.0078082 , 0.00799738],\n",
       "       [0.00643931, 0.00662849, 0.00642682, 0.00664455]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.math.l2_normalize(\n",
    "    X_np_array,\n",
    "    axis=None,\n",
    "    epsilon=1e-12,\n",
    "    name=None\n",
    ")\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmXUGc2oTspa"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJelDMpzxs0L"
   },
   "source": [
    "### Define weight and bias\n",
    "- Initialize weight and bias with tf.zeros\n",
    "- tf.zeros is an initializer that generates tensors initialized to 0\n",
    "- Specify the value for shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8o9RPWVTxs0O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]], shape=(4, 1), dtype=float32)\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = tf.zeros(shape=(4, 1)) \n",
    "b = tf.zeros(shape=(1))\n",
    "\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a0wr94aTyjg"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMXXYdOSxs0Q"
   },
   "source": [
    "### Get prediction\n",
    "- Define a function to get prediction\n",
    "- Approach: prediction = (X * W) + b; here is X is features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8Cty1y0xs0S"
   },
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def prediction(features, W, b):     \n",
    "  y_pred = tf.add(tf.matmul(features, W), b)     \n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQmS3Tauxs0V"
   },
   "source": [
    "### Calculate loss\n",
    "- Calculate loss using predictions\n",
    "- Define a function to calculate loss\n",
    "- We are calculating mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FRXmDd5xs0X"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss(y_actual, y_predicted):\n",
    " diff = y_actual - y_predicted   \n",
    " sqr = tf.square(diff)     \n",
    " avg = tf.reduce_mean(sqr)     \n",
    " return avg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbBpnOtfT0wd"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkOzAUUsTmF_"
   },
   "source": [
    "### Define a function to train the model\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R4uieGYLYtM"
   },
   "outputs": [],
   "source": [
    "def train(X, y_actual, w, b, learning_rate=0.01):\n",
    "  # Record mathematical operations on 'tape' to calculate loss     \n",
    "  with tf.GradientTape() as t:         \n",
    "    t.watch([w,b])         \n",
    "    current_prediction = prediction(X, w, b)         \n",
    "    current_loss = loss(y_actual, current_prediction)          \n",
    "  # Calculate Gradients for Loss with respect to Weights and Bias     \n",
    "  dw, db = t.gradient(current_loss,[w, b])          \n",
    "  # Update Weights and Bias     \n",
    "  w = w - learning_rate * dw     \n",
    "  b = b - learning_rate * db          \n",
    "  return w, b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW4SEP8kT2ls"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeN0deOvT81N"
   },
   "source": [
    "### Train the model for 100 epochs \n",
    "- Observe the training loss at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jjkn4gUgLevE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.02202894 0.02245906 0.02182905 0.02253223]\n",
      " [0.02235197 0.02141321 0.02140607 0.02240552]\n",
      " [0.0207707  0.02051548 0.02051192 0.02137037]\n",
      " ...\n",
      " [0.00505436 0.00513467 0.00499903 0.00514181]\n",
      " [0.00785282 0.0079956  0.0078082  0.00799738]\n",
      " [0.00643931 0.00662849 0.00642682 0.00664455]], shape=(1000, 4), dtype=float32)\n",
      "Current Training Loss on iteration 0 1.6270466e-10\n",
      "Current Training Loss on iteration 1 1.5889905e-10\n",
      "Current Training Loss on iteration 2 1.5524503e-10\n",
      "Current Training Loss on iteration 3 1.517352e-10\n",
      "Current Training Loss on iteration 4 1.4836452e-10\n",
      "Current Training Loss on iteration 5 1.451275e-10\n",
      "Current Training Loss on iteration 6 1.4201873e-10\n",
      "Current Training Loss on iteration 7 1.3903309e-10\n",
      "Current Training Loss on iteration 8 1.3616565e-10\n",
      "Current Training Loss on iteration 9 1.3341202e-10\n",
      "Current Training Loss on iteration 10 1.307674e-10\n",
      "Current Training Loss on iteration 11 1.2822761e-10\n",
      "Current Training Loss on iteration 12 1.2578831e-10\n",
      "Current Training Loss on iteration 13 1.2344595e-10\n",
      "Current Training Loss on iteration 14 1.2119616e-10\n",
      "Current Training Loss on iteration 15 1.1903561e-10\n",
      "Current Training Loss on iteration 16 1.1696061e-10\n",
      "Current Training Loss on iteration 17 1.1496787e-10\n",
      "Current Training Loss on iteration 18 1.1305407e-10\n",
      "Current Training Loss on iteration 19 1.1121612e-10\n",
      "Current Training Loss on iteration 20 1.0945095e-10\n",
      "Current Training Loss on iteration 21 1.0775572e-10\n",
      "Current Training Loss on iteration 22 1.061277e-10\n",
      "Current Training Loss on iteration 23 1.0456417e-10\n",
      "Current Training Loss on iteration 24 1.0306261e-10\n",
      "Current Training Loss on iteration 25 1.01620504e-10\n",
      "Current Training Loss on iteration 26 1.0023557e-10\n",
      "Current Training Loss on iteration 27 9.8905495e-11\n",
      "Current Training Loss on iteration 28 9.762809e-11\n",
      "Current Training Loss on iteration 29 9.640133e-11\n",
      "Current Training Loss on iteration 30 9.522321e-11\n",
      "Current Training Loss on iteration 31 9.4091734e-11\n",
      "Current Training Loss on iteration 32 9.300511e-11\n",
      "Current Training Loss on iteration 33 9.196151e-11\n",
      "Current Training Loss on iteration 34 9.095927e-11\n",
      "Current Training Loss on iteration 35 8.999673e-11\n",
      "Current Training Loss on iteration 36 8.907236e-11\n",
      "Current Training Loss on iteration 37 8.8184585e-11\n",
      "Current Training Loss on iteration 38 8.733199e-11\n",
      "Current Training Loss on iteration 39 8.651319e-11\n",
      "Current Training Loss on iteration 40 8.572688e-11\n",
      "Current Training Loss on iteration 41 8.497162e-11\n",
      "Current Training Loss on iteration 42 8.4246325e-11\n",
      "Current Training Loss on iteration 43 8.354982e-11\n",
      "Current Training Loss on iteration 44 8.288088e-11\n",
      "Current Training Loss on iteration 45 8.2238424e-11\n",
      "Current Training Loss on iteration 46 8.1621425e-11\n",
      "Current Training Loss on iteration 47 8.102889e-11\n",
      "Current Training Loss on iteration 48 8.045982e-11\n",
      "Current Training Loss on iteration 49 7.991331e-11\n",
      "Current Training Loss on iteration 50 7.938847e-11\n",
      "Current Training Loss on iteration 51 7.888441e-11\n",
      "Current Training Loss on iteration 52 7.840029e-11\n",
      "Current Training Loss on iteration 53 7.793538e-11\n",
      "Current Training Loss on iteration 54 7.74889e-11\n",
      "Current Training Loss on iteration 55 7.70601e-11\n",
      "Current Training Loss on iteration 56 7.664826e-11\n",
      "Current Training Loss on iteration 57 7.625279e-11\n",
      "Current Training Loss on iteration 58 7.587298e-11\n",
      "Current Training Loss on iteration 59 7.550819e-11\n",
      "Current Training Loss on iteration 60 7.515788e-11\n",
      "Current Training Loss on iteration 61 7.4821455e-11\n",
      "Current Training Loss on iteration 62 7.449834e-11\n",
      "Current Training Loss on iteration 63 7.418801e-11\n",
      "Current Training Loss on iteration 64 7.389e-11\n",
      "Current Training Loss on iteration 65 7.36038e-11\n",
      "Current Training Loss on iteration 66 7.332892e-11\n",
      "Current Training Loss on iteration 67 7.306495e-11\n",
      "Current Training Loss on iteration 68 7.2811444e-11\n",
      "Current Training Loss on iteration 69 7.256797e-11\n",
      "Current Training Loss on iteration 70 7.233413e-11\n",
      "Current Training Loss on iteration 71 7.21096e-11\n",
      "Current Training Loss on iteration 72 7.189395e-11\n",
      "Current Training Loss on iteration 73 7.168681e-11\n",
      "Current Training Loss on iteration 74 7.1487906e-11\n",
      "Current Training Loss on iteration 75 7.1296906e-11\n",
      "Current Training Loss on iteration 76 7.1113435e-11\n",
      "Current Training Loss on iteration 77 7.093722e-11\n",
      "Current Training Loss on iteration 78 7.076802e-11\n",
      "Current Training Loss on iteration 79 7.060553e-11\n",
      "Current Training Loss on iteration 80 7.0449445e-11\n",
      "Current Training Loss on iteration 81 7.029957e-11\n",
      "Current Training Loss on iteration 82 7.0155645e-11\n",
      "Current Training Loss on iteration 83 7.001739e-11\n",
      "Current Training Loss on iteration 84 6.988464e-11\n",
      "Current Training Loss on iteration 85 6.975714e-11\n",
      "Current Training Loss on iteration 86 6.963468e-11\n",
      "Current Training Loss on iteration 87 6.9517093e-11\n",
      "Current Training Loss on iteration 88 6.940413e-11\n",
      "Current Training Loss on iteration 89 6.929567e-11\n",
      "Current Training Loss on iteration 90 6.91915e-11\n",
      "Current Training Loss on iteration 91 6.909149e-11\n",
      "Current Training Loss on iteration 92 6.899541e-11\n",
      "Current Training Loss on iteration 93 6.890312e-11\n",
      "Current Training Loss on iteration 94 6.88145e-11\n",
      "Current Training Loss on iteration 95 6.87294e-11\n",
      "Current Training Loss on iteration 96 6.864769e-11\n",
      "Current Training Loss on iteration 97 6.8569185e-11\n",
      "Current Training Loss on iteration 98 6.84938e-11\n",
      "Current Training Loss on iteration 99 6.842143e-11\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "for i in range(100):         \n",
    "    #W, b = train(X, Y_arr, W, b)   \n",
    "    W, b = train(X,Y_np_array, W, b)  \n",
    "    print('Current Training Loss on iteration', i, loss(Y_np_array, prediction(X, W, b)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vanvD93FV0_k"
   },
   "source": [
    "### Observe values of Weight\n",
    "- Print the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QSqpy4gtWaOD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5609, shape=(4, 1), dtype=float32, numpy=\n",
       "array([[1.03087160e-07],\n",
       "       [1.03273656e-07],\n",
       "       [1.02041902e-07],\n",
       "       [1.04118989e-07]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9KpRupYUEwy"
   },
   "source": [
    "### Observe values of Bias\n",
    "- Print the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhEWkGqHWohg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5612, shape=(1,), dtype=float32, numpy=array([8.670426e-06], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Questions - Internal - R6 - AIML Labs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
